{
  "2510.14880v1": {
    "title": "Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report",
    "url": "https://www.alphaxiv.org/abs/2510.14880v1",
    "arxiv_id": "2510.14880v1",
    "authors": "Rikiya Takehi, Benjamin Clavié, Sean Lee, Aamir Shakir",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 17:00:35",
    "ori_summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
    "summary": "该论文研究如何训练高效的小型检索模型，核心思想是通过知识蒸馏和消融研究开发参数极少的ColBERT变体模型，实现从云端到边缘设备的全尺度检索能力。",
    "translation": "神奇（小型）检索器及其训练方法：mxbai-edge-colbert-v0 技术报告",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于小型高效检索器的训练，这直接属于搜索领域的核心进展。虽然论文标题未明确提及LLM，但ColBERT架构是检索增强生成(RAG)中的关键技术，在搜索和推荐系统中具有直接应用价值，能够提升检索效率和准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文专注于小型检索模型的训练方法，直接涉及检索系统的核心技术进步，特别是边缘设备的检索能力，与搜索和推荐系统的实际部署高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14857v1": {
    "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems",
    "url": "https://www.alphaxiv.org/abs/2510.14857v1",
    "arxiv_id": "2510.14857v1",
    "authors": "Gabriele Barlacchi, Margherita Lalli, Emanuele Ferragina, Fosca Giannotti, Luca Pappalardo",
    "categories": "cs.IR, cs.CY",
    "pub_date": "2025-10-16 16:31:01",
    "ori_summary": "Recommender systems continuously interact with users, creating feedback loops that shape both individual behavior and collective market dynamics. This paper introduces a simulation framework to model these loops in online retail environments, where recommenders are periodically retrained on evolving user-item interactions. Using the Amazon e-Commerce dataset, we analyze how different recommendation algorithms influence diversity, purchase concentration, and user homogenization over time. Results reveal a systematic trade-off: while the feedback loop increases individual diversity, it simultaneously reduces collective diversity and concentrates demand on a few popular items. Moreover, for some recommender systems, the feedback loop increases user homogenization over time, making user purchase profiles increasingly similar. These findings underscore the need for recommender designs that balance personalization with long-term diversity.",
    "summary": "该论文研究推荐系统中反馈循环对用户行为和市场动态的系统性影响，核心思想是通过仿真框架分析推荐算法在持续重训练过程中如何影响多样性和用户同质化，揭示了个体多样性与集体多样性之间的权衡关系。",
    "translation": "研究推荐系统中反馈循环系统性影响的仿真框架",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统核心机制中的反馈循环问题，这是RecSys领域的关键挑战。仿真框架能够帮助理解推荐算法如何影响用户行为和数据分布，对于改进推荐系统的长期性能和稳定性具有重要价值。这种系统性分析方法可以应用于搜索和广告系统的优化。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究推荐系统中的反馈循环效应，属于推荐系统核心领域进展，对理解推荐算法长期影响有重要意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14824v1": {
    "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.14824v1",
    "arxiv_id": "2510.14824v1",
    "authors": "Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang",
    "categories": "cs.CL, cs.CV, cs.IR",
    "pub_date": "2025-10-16 16:02:27",
    "ori_summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
    "summary": "该论文研究大型语言模型在信息检索重排序任务中不同训练目标的选择问题，核心思想是通过分解权重和方向两个组件，系统分析监督微调相比对比学习在LLM重排序中的内在优势机制。",
    "translation": "监督微调还是对比学习？迈向更好的多模态大语言模型重排序",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及多模态LLM在重排序任务中的应用，这直接属于'Direct LLM Applications'范畴，对搜索和推荐系统具有重要价值。对比学习和监督微调的比较研究可以为搜索和推荐中的多模态内容重排序提供实用的技术指导，提升排序质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接比较监督微调与对比学习在LLM重排序中的效果，为核心LLM技术在搜索领域的应用提供了重要指导。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14788v1": {
    "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
    "url": "https://www.alphaxiv.org/abs/2510.14788v1",
    "arxiv_id": "2510.14788v1",
    "authors": "Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 15:20:49",
    "ori_summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
    "summary": "论文研究跨场景用户兴趣统一建模问题，核心方法是构建LLM增强的层次化推荐框架，通过双塔架构和多场景行为信号融合来捕获跨场景用户意图模式。",
    "translation": "十亿规模用户兴趣的跨场景统一建模",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统的核心挑战——大规模用户兴趣建模，属于Core Domain Advances范畴。跨场景统一建模方法可应用于搜索和广告中的用户理解，同时这种统一建模思想与VLM Analogy for Heterogeneous Data的理念高度契合，将不同场景数据视为不同模态进行统一处理。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对跨场景统一用户建模这一推荐系统核心问题，采用LLM增强的层次化框架处理异构行为数据，与关注领域高度契合。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14704v1": {
    "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?",
    "url": "https://www.alphaxiv.org/abs/2510.14704v1",
    "arxiv_id": "2510.14704v1",
    "authors": "Leonie Winter",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 14:08:30",
    "ori_summary": "Offline evaluations in recommender system research depend heavily on datasets, many of which are pruned, such as the widely used MovieLens collections. This thesis examines the impact of data pruning - specifically, removing users with fewer than a specified number of interactions - on both dataset characteristics and algorithm performance. Five benchmark datasets were analysed in both their unpruned form and at five successive pruning levels (5, 10, 20, 50, 100). For each coreset, we examined structural and distributional characteristics and trained and tested eleven representative algorithms. To further assess if pruned datasets lead to artificially inflated performance results, we also evaluated models trained on the pruned train sets but tested on unpruned data. Results show that commonly applied core pruning can be highly selective, leaving as little as 2% of the original users in some datasets. Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets. Across all algorithms, performance declined with increasing pruning levels when tested on unpruned data, highlighting the impact of dataset reduction on the performance of recommender algorithms.",
    "summary": "论文研究推荐系统中数据集修剪对算法性能评估的影响问题，核心发现是数据集修剪会改变数据分布特性，导致在修剪数据上评估的性能指标可能无法反映在完整数据上的真实表现。",
    "translation": "推荐系统和机器学习中的数据集剪枝：最佳实践还是不当实践？",
    "relevance_score": 8,
    "reasoning": "该论文直接聚焦于推荐系统（RecSys）领域的核心数据预处理技术——数据集剪枝，这属于'核心领域进展'范畴。数据集剪枝技术对于提升推荐系统训练效率和模型性能具有直接影响，能够优化大规模推荐系统的数据处理流程和模型训练效果。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究推荐系统数据集修剪的核心实践问题，揭示了修剪对算法性能评估的潜在误导性影响，对推荐系统研究方法和基准评估具有重要指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14670v1": {
    "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.14670v1",
    "arxiv_id": "2510.14670v1",
    "authors": "Marco Simoni, Aleksandar Fontana, Andrea Saracino, Paolo Mori",
    "categories": "cs.AI, cs.CL, cs.CR, cs.IR",
    "pub_date": "2025-10-16 13:27:05",
    "ori_summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
    "summary": "",
    "translation": "TITAN：面向网络威胁情报的图可执行推理",
    "relevance_score": 1,
    "reasoning": "该论文专注于网络安全领域的威胁情报分析，属于完全无关的领域特定应用。论文标题中提到的图推理与网络威胁情报直接相关，没有任何内容表明与推荐系统、搜索或广告有潜在联系，完全超出了当前关注的技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14660v1": {
    "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14660v1",
    "arxiv_id": "2510.14660v1",
    "authors": "Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 13:15:40",
    "ori_summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
    "summary": "",
    "translation": "一种基于评分准则的高效生成式验证器，用于搜索增强型大语言模型",
    "relevance_score": 3,
    "reasoning": "该论文主要关注搜索增强型LLMs的验证机制，这属于LLM评估和可靠性的范畴，与纯粹的推荐系统、搜索或广告排名核心进展关联较弱。虽然提到了搜索增强，但焦点在于验证而非搜索本身的改进，且未明确涉及推荐或广告领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14641v1": {
    "title": "Causality Enhancement for Cross-Domain Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14641v1",
    "arxiv_id": "2510.14641v1",
    "authors": "Zhibo Wu, Yunfan Wu, Lin Jiang, Ping Yang, Yao Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 12:54:46",
    "ori_summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
    "summary": "论文研究跨域推荐中源域信息利用不足的问题，核心思想是通过构建因果图指导、启发式构建因果感知数据集，并设计理论无偏的部分标签因果损失来增强跨域表示学习。",
    "translation": "跨域推荐的因果性增强",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统（RecSys）的核心领域进展，专注于跨域推荐这一重要问题。通过因果性增强方法，可以显著提升推荐系统的性能和可解释性，这属于推荐系统领域的关键技术改进。因果推理在推荐系统中具有广泛应用，能够解决数据偏差、提升推荐质量，并增强用户理解。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的核心跨域推荐问题，提出了因果增强框架，属于推荐系统领域的核心进展，与LLM无关但高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14640v1": {
    "title": "Intent Clustering with Shared Pseudo-Labels",
    "url": "https://www.alphaxiv.org/abs/2510.14640v1",
    "arxiv_id": "2510.14640v1",
    "authors": "I-Fan Lin, Faegheh Hasibi, Suzan Verberne",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-16 12:54:40",
    "ori_summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
    "summary": "",
    "translation": "基于共享伪标签的意图聚类",
    "relevance_score": 7,
    "reasoning": "意图聚类是搜索和推荐系统中的核心任务，用于理解用户查询意图并改进内容匹配。该方法通过共享伪标签实现聚类，可应用于用户行为分析、查询理解、兴趣发现等场景，直接服务于搜索和推荐系统的意图建模需求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14629v1": {
    "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14629v1",
    "arxiv_id": "2510.14629v1",
    "authors": "Jiani Huang, Xingchen Zou, Lianghao Xia, Qing Li",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 12:40:48",
    "ori_summary": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.",
    "summary": "论文研究LLM在推荐系统中如何实现深度个性化和智能推理的核心问题，核心方法是设计了一个结合检索增强生成与强化学习的框架，通过动态记忆检索和自适应推理的协同作用来提升推荐质量。",
    "translation": "MR.Rec：基于大语言模型的个性化推荐助手，协同融合记忆与推理能力",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术构建个性化推荐系统，属于'直接LLM应用'范畴。通过协同融合记忆（用户历史行为）与推理（LLM的推理能力）来增强推荐效果，这种架构设计对搜索和广告推荐系统具有直接的应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的核心挑战，提出了结合记忆与推理的创新框架，完美契合个性化推荐和LLM应用的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14626v1": {
    "title": "GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14626v1",
    "arxiv_id": "2510.14626v1",
    "authors": "Zhibo Wu, Yunfan Wu, Quan Liu, Lin Jiang, Ping Yang, Yao Hu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 12:37:15",
    "ori_summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
    "summary": "该论文研究多兴趣推荐中兴趣塌陷和演化建模不足的核心问题，核心方法是提出GemiRec框架，通过兴趣量化强制结构分离和兴趣生成显式学习用户兴趣演化动态。",
    "translation": "GemiRec：多兴趣推荐中的兴趣量化与生成",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及多兴趣推荐系统，这是推荐系统领域的核心进展。兴趣量化和生成技术可以显著提升用户建模的准确性和多样性，在电商、内容推荐等场景中有直接应用价值。该方法通过更精细的用户兴趣表示来改进推荐质量，完全符合核心推荐系统技术的研究范畴。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的核心挑战——多兴趣建模，提出了框架级解决方案，通过兴趣量化和生成机制解决兴趣塌陷和演化建模不足的问题，与工业检索阶段高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14592v1": {
    "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.14592v1",
    "arxiv_id": "2510.14592v1",
    "authors": "Rashmi R, Vidyadhar Upadhya",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2025-10-16 11:55:24",
    "ori_summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
    "summary": "该论文研究多模态非结构化文档的检索增强生成问题，核心方法是构建模态感知知识图谱来编码跨模态语义关系，并设计混合检索架构结合稠密向量检索和图遍历实现上下文感知的跨模态检索。",
    "translation": "面向非结构化数据的多模态检索增强生成：利用模态感知知识图谱与混合检索",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态RAG和知识图谱技术，属于核心LLM技术进展，具有在搜索和推荐系统中应用的明确潜力。模态感知知识图谱可以统一建模用户行为序列和内容特征等异构数据，类似于VLM处理不同模态的方法，混合检索机制可直接提升搜索和推荐系统的检索质量与效率。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的多模态感知知识图谱和混合检索架构直接对应VLM类比异构数据的焦点，将不同模态视为统一建模对象，在检索增强生成框架中实现了跨模态语义理解。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14545v1": {
    "title": "Agentic Entropy-Balanced Policy Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.14545v1",
    "arxiv_id": "2510.14545v1",
    "authors": "Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou",
    "categories": "cs.LG, cs.AI, cs.CL, cs.IR",
    "pub_date": "2025-10-16 10:40:52",
    "ori_summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
    "summary": "",
    "translation": "智能体熵平衡策略优化",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于强化学习中的策略优化方法，涉及熵平衡技术。虽然强化学习在推荐系统和广告中有潜在应用，但标题本身没有明确指向RecSys/Search/Ads领域，也没有提及LLM、Transformer或异构数据建模等核心技术。需要更多上下文来确定其具体应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14535v1": {
    "title": "Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval",
    "url": "https://www.alphaxiv.org/abs/2510.14535v1",
    "arxiv_id": "2510.14535v1",
    "authors": "Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-16 10:27:21",
    "ori_summary": "Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.",
    "summary": "",
    "translation": "基于内容的脑部MR图像检索中可解释性领域信息的获取",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像（脑部MR）的协调化和基于内容的图像检索，属于医学影像处理领域。虽然涉及检索概念，但其应用场景（医学诊断）和技术重点（图像协调化）与推荐系统、搜索或广告的核心技术需求完全无关，且不涉及任何LLM、Transformer或推荐系统相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14400v1": {
    "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14400v1",
    "arxiv_id": "2510.14400v1",
    "authors": "Yingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua Wang, Yuchen Pan, Hongfei Lin",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 07:59:11",
    "ori_summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
    "summary": "",
    "translation": "MedTrust-RAG：面向生物医学问答的证据验证与信任对齐",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物医学领域的问答系统，属于明确的医学领域应用，属于无关主题范畴。虽然提到了RAG技术，但其核心应用场景（生物医学问答）与推荐系统、搜索或广告领域没有直接关联，且没有显示出在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14377v1": {
    "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
    "url": "https://www.alphaxiv.org/abs/2510.14377v1",
    "arxiv_id": "2510.14377v1",
    "authors": "Mykolas Sveistrys, Richard Kunert",
    "categories": "cs.CL, cs.IR, cs.LG",
    "pub_date": "2025-10-16 07:22:58",
    "ori_summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
    "summary": "",
    "translation": "PluriHop：在干扰项丰富的语料库上进行穷尽式、召回敏感的问答",
    "relevance_score": 2,
    "reasoning": "该论文主要关注问答系统在干扰项丰富环境下的召回性能，属于通用NLP领域。虽然问答技术与搜索系统有一定关联，但论文没有明确涉及推荐系统、广告或Transformer架构改进，且缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14330v1": {
    "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations",
    "url": "https://www.alphaxiv.org/abs/2510.14330v1",
    "arxiv_id": "2510.14330v1",
    "authors": "Yuto Nakamizo, Ryuhei Miyazato, Hikaru Tanabe, Ryuta Yamakura, Kiori Hatanaka",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 06:09:26",
    "ori_summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
    "summary": "",
    "translation": "基于视觉语言大模型内部表征训练的多幻觉检测器集成",
    "relevance_score": 1,
    "reasoning": "该论文专注于幻觉检测这一纯粹的NLP中心话题，属于明确的无关主题范畴。虽然涉及VLLM技术，但核心关注点（幻觉检测）与推荐系统、搜索或广告领域的技术进步没有直接关联，且无法看出在相关领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14321v1": {
    "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm",
    "url": "https://www.alphaxiv.org/abs/2510.14321v1",
    "arxiv_id": "2510.14321v1",
    "authors": "Jianting Tang, Dongshuai Li, Tao Wen, Fuyu Lv, Dan Ou, Linli Xu",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 05:37:39",
    "ori_summary": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
    "summary": "论文研究电商搜索中语义鸿沟导致的检索性能下降问题，核心思想是通过两阶段训练让LLM先对困难查询进行深度推理，再生成推理增强的查询嵌入来弥合语义差距。",
    "translation": "大型推理嵌入模型：迈向下一代稠密检索范式",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及稠密检索范式的核心进展，这是搜索和推荐系统的关键技术。大型推理嵌入模型通过增强语义理解和推理能力，可以显著提升搜索相关性排序和个性化推荐质量，代表了检索技术的重要发展方向。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的核心检索问题，提出将推理过程融入表示学习的新范式，完美契合核心领域进展和直接LLM应用两大焦点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14296v1": {
    "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL",
    "url": "https://www.alphaxiv.org/abs/2510.14296v1",
    "arxiv_id": "2510.14296v1",
    "authors": "Md Mahadi Hasan Nahid, Davood Rafiei, Weiwei Zhang, Yong Zhang",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-16 04:46:22",
    "ori_summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
    "summary": "",
    "translation": "重新思考模式链接：面向文本到SQL的上下文感知双向检索方法",
    "relevance_score": 3,
    "reasoning": "该论文主要关注文本到SQL转换中的模式链接问题，这属于数据库查询优化的特定领域。虽然涉及检索技术，但其应用场景局限于结构化数据库查询，与推荐系统、搜索或广告中的核心排名和个性化问题关联度较低。双向检索方法可能对搜索中的查询理解有一定启发，但整体相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14278v1": {
    "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14278v1",
    "arxiv_id": "2510.14278v1",
    "authors": "Md Mahadi Hasan Nahid, Davood Rafiei",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-16 04:02:29",
    "ori_summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
    "summary": "论文研究多跳问答中的证据检索问题，核心方法是设计由问题分析器、选择器和补充器三个智能体组成的LLM驱动框架，通过迭代交互实现精准且全面的证据检索。",
    "translation": "PRISM：基于大语言模型的智能检索用于多跳问答",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM驱动的智能检索系统，这直接适用于搜索领域的检索增强生成(RAG)应用。多跳问答的检索机制可以类比为推荐系统中的多阶段排序，其中LLM作为智能代理协调复杂的检索流程，对搜索和推荐系统的架构设计具有重要参考价值。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "论文提出基于LLM的智能检索框架，通过多智能体协作解决多跳问答中的证据检索问题，直接应用于搜索领域且涉及LLM代理技术。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14257v1": {
    "title": "Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.14257v1",
    "arxiv_id": "2510.14257v1",
    "authors": "Lingyu Mu, Hao Deng, Haibo Xing, Kaican Lin, Zhitong Zhu, Yu Zhang, Xiaoyi Zeng, Zhengxiao Liu, Zheng Lin, Jinxin Hu",
    "categories": "cs.IR",
    "pub_date": "2025-10-16 03:16:21",
    "ori_summary": "The integration of large language models (LLMs) into recommendation systems has revealed promising potential through their capacity to extract world knowledge for enhanced reasoning capabilities. However, current methodologies that adopt static schema-based prompting mechanisms encounter significant limitations: (1) they employ universal template structures that neglect the multi-faceted nature of user preference diversity; (2) they implement superficial alignment between semantic knowledge representations and behavioral feature spaces without achieving comprehensive latent space integration. To address these challenges, we introduce CoCo, an end-to-end framework that dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach. Our method realizes profound integration of semantic and behavioral latent dimensions via adaptive knowledge fusion and contradiction resolution modules. Experimental evaluations across diverse benchmark datasets and an enterprise-level e-commerce platform demonstrate CoCo's superiority, achieving a maximum 8.58% improvement over seven cutting-edge methods in recommendation accuracy. The framework's deployment on a production advertising system resulted in a 1.91% sales growth, validating its practical effectiveness. With its modular design and model-agnostic architecture, CoCo provides a versatile solution for next-generation recommendation systems requiring both knowledge-enhanced reasoning and personalized adaptation.",
    "summary": "论文研究LLM在推荐系统中知识集成不足的问题，核心思想是通过动态构建用户特定上下文知识嵌入，结合自适应知识融合和矛盾解决模块实现语义与行为潜在空间的深度整合。",
    "translation": "面向个性化推荐的上下文知识协同集成与差异消解",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦个性化推荐系统的核心问题，涉及上下文知识的集成与差异处理，这属于Core Domain Advances范畴。论文标题暗示了处理多源上下文信息的方法，这种技术可以应用于搜索和广告中的上下文感知排序，通过更好地整合用户上下文、项目特征和交互历史来提升推荐质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的核心挑战，提出动态上下文知识集成和矛盾解决机制，完美契合直接LLM应用和异构数据统一建模的研究重点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14223v1": {
    "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14223v1",
    "arxiv_id": "2510.14223v1",
    "authors": "Sudarshan Srinivasa Ramanujam, Antonio Alonso, Saurabh Kataria, Siddharth Dangi, Akhilesh Gupta, Birjodh Singh Tiwana, Manas Somaiya, Luke Simon, David Byrne, Sojeong Ha, Sen Zhou, Andrei Akterskii, Zhanglong Liu, Samira Sriram, Crescent Xiong, Zhoutao Pei, Angela Shao, Alex Li, Annie Xiao, Caitlin Kolb, Thomas Kistler, Zach Moore, Hamed Firooz",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-16 02:01:33",
    "ori_summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval stage is critical for narrowing hundreds of millions of potential candidates to a manageable subset for ranking. LinkedIn's Feed serves suggested content from outside of the member's network (based on the member's topical interests), where 2000 candidates are retrieved from a pool of hundreds of millions candidate with a latency budget of a few milliseconds and inbound QPS of several thousand per second. This paper presents a novel retrieval approach that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual encoder to generate high quality embeddings for both users (members) and content (items), using only textual input. We describe the end to end pipeline, including prompt design for embedding generation, techniques for fine-tuning at LinkedIn's scale, and infrastructure for low latency, cost effective online serving. We share our findings on how quantizing numerical features in the prompt enables the information to get properly encoded in the embedding, facilitating greater alignment between the retrieval and ranking layer. The system was evaluated using offline metrics and an online A/B test, which showed substantial improvements in member engagement. We observed significant gains among newer members, who often lack strong network connections, indicating that high-quality suggested content aids retention. This work demonstrates how generative language models can be effectively adapted for real time, high throughput retrieval in industrial applications.",
    "summary": "该论文研究大规模推荐系统中从海量候选中高效召回的问题，核心方法是微调因果语言模型作为双编码器，仅使用文本输入为用户和内容生成高质量嵌入。",
    "translation": "基于因果语言模型的大规模LinkedIn信息流检索",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统（LinkedIn Feed）中的大规模检索问题，这是核心推荐系统领域的核心进展。因果语言模型作为LLM技术，在推荐系统中具有直接应用潜力，可用于建模用户行为序列的因果关系，提升检索质量和个性化推荐效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用因果语言模型于大规模推荐系统的召回阶段，完全符合核心领域进展和直接LLM应用的重点方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14980v1": {
    "title": "Agentic Design of Compositional Machines",
    "url": "https://www.alphaxiv.org/abs/2510.14980v1",
    "arxiv_id": "2510.14980v1",
    "authors": "Wenqian Zhang, Weiyang Liu, Zhen Liu",
    "categories": "cs.AI, cs.CL, cs.CV, cs.GR, cs.LG",
    "pub_date": "2025-10-16 17:59:58",
    "ori_summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
    "summary": "",
    "translation": "组合机器的代理化设计",
    "relevance_score": 2,
    "reasoning": "该标题涉及代理设计和组合机器，但缺乏与推荐系统、搜索或广告的具体联系。虽然代理系统在理论上可以应用于用户交互，但标题过于宽泛，没有明确指向LLM技术、Transformer架构或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14973v1": {
    "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14973v1",
    "arxiv_id": "2510.14973v1",
    "authors": "Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:48",
    "ori_summary": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences, and $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
    "summary": "",
    "translation": "注意力机制是扩散大语言模型中KV缓存的全部所需",
    "relevance_score": 7,
    "reasoning": "该论文涉及Transformer架构中的注意力机制优化和KV缓存效率提升，这属于'Enabling Transformer Tech'范畴。高效的KV缓存技术可以显著降低推理延迟和内存消耗，这对于需要实时响应的推荐系统和搜索应用至关重要，能够支持更大模型在线上服务中的部署。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14972v1": {
    "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
    "url": "https://www.alphaxiv.org/abs/2510.14972v1",
    "arxiv_id": "2510.14972v1",
    "authors": "Yinxi Li, Yuntian Deng, Pengyu Nie",
    "categories": "cs.CL, cs.AI, cs.LG, cs.PL, cs.SE",
    "pub_date": "2025-10-16 17:59:45",
    "ori_summary": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
    "summary": "",
    "translation": "TokDrift：当大语言模型以子词说话而代码以语法说话时",
    "relevance_score": 2,
    "reasoning": "该论文探讨LLM分词与代码语法之间的不匹配问题，这属于核心LLM技术中的分词挑战。虽然分词技术是LLM的基础组件，但论文主要关注代码理解领域，与推荐系统、搜索或广告的直接应用关联较弱，仅在提升代码相关搜索质量方面有间接潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14969v1": {
    "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
    "url": "https://www.alphaxiv.org/abs/2510.14969v1",
    "arxiv_id": "2510.14969v1",
    "authors": "Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:38",
    "ori_summary": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.",
    "summary": "论文研究数字代理训练中大规模UI轨迹数据获取困难的问题，核心思想是利用LLM构建可扩展的UI模拟器，通过结构化状态生成和引导式探索来合成高质量训练轨迹。",
    "translation": "LLM作为可扩展的通用模拟器用于演化数字智能体训练",
    "relevance_score": 8,
    "reasoning": "该论文涉及使用LLM作为模拟器来训练数字智能体，这属于'直接LLM应用'范畴，在推荐系统和搜索领域有直接应用潜力，可用于模拟用户行为、测试推荐策略或训练会话搜索智能体。虽然标题未明确提及RecSys/Search/Ads，但数字智能体训练的概念与这些领域的交互式系统高度相关。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出利用LLM作为模拟器生成UI轨迹数据的方法，直接应用于数字代理训练，属于LLM在推荐/搜索系统中数据生成和增强的直接应用范畴。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14967v1": {
    "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14967v1",
    "arxiv_id": "2510.14967v1",
    "authors": "Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:32",
    "ori_summary": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
    "summary": "论文研究多轮LLM智能体训练中的奖励稀疏问题，核心思想是将每轮交互建模为信息获取过程，通过模型自身信念更新的概率增益来定义轮级内在奖励。",
    "translation": "基于信息增益的策略优化：一种简单有效的多轮LLM智能体方法",
    "relevance_score": 8,
    "reasoning": "该论文属于'直接LLM应用'类别，专注于多轮LLM智能体的策略优化，这在搜索和推荐系统中具有直接应用价值。多轮交互是搜索对话系统和推荐会话系统的核心场景，信息增益方法可以优化智能体在复杂交互中的决策过程，提升用户体验和系统性能。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出基于信息增益的强化学习框架，直接解决多轮交互中的奖励稀疏问题，对搜索和推荐系统中的序列决策具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14961v1": {
    "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14961v1",
    "arxiv_id": "2510.14961v1",
    "authors": "Jonas Geiping, Xinyu Yang, Guinan Su",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-16 17:59:07",
    "ori_summary": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
    "summary": "",
    "translation": "用于循环深度模型的高效并行采样器及其与扩散语言模型的联系",
    "relevance_score": 3,
    "reasoning": "该论文主要关注循环深度模型的采样效率和扩散模型的连接，属于底层模型架构和采样技术。虽然这些技术可能间接应用于推荐系统或搜索中的序列建模，但论文标题没有明确表明在RecSys/Search/Ads领域的直接应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14958v1": {
    "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14958v1",
    "arxiv_id": "2510.14958v1",
    "authors": "Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 17:58:58",
    "ori_summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
    "summary": "",
    "translation": "MathCanvas：用于多模态数学推理的内在视觉思维链",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态数学推理和视觉思维链，属于视觉-语言交互的特定领域应用。虽然涉及多模态建模，但其核心应用场景（数学推理）与推荐系统、搜索或广告领域没有直接关联，也不涉及这些领域所需的核心技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14949v1": {
    "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14949v1",
    "arxiv_id": "2510.14949v1",
    "authors": "Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng",
    "categories": "cs.CL, cs.CV, cs.LG",
    "pub_date": "2025-10-16 17:56:55",
    "ori_summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
    "summary": "",
    "translation": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态生成中的方言鲁棒性，属于纯粹的NLP和多模态研究领域。虽然涉及基准测试，但这属于评估基准范畴，且没有展示在推荐系统、搜索或广告中的潜在应用价值。方言处理在RecSys/Search/Ads中的相关性极低，属于明确的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14944v1": {
    "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
    "url": "https://www.alphaxiv.org/abs/2510.14944v1",
    "arxiv_id": "2510.14944v1",
    "authors": "Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang",
    "categories": "cs.CL, cs.AI, cs.CE",
    "pub_date": "2025-10-16 17:55:14",
    "ori_summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.",
    "summary": "",
    "translation": "MetaBench：用于评估代谢组学中大型语言模型的多任务基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于代谢组学领域的LLM评估基准，这属于生物学/医学领域的特定应用，与搜索、推荐或广告系统完全无关。论文标题明确指向生物医学应用场景，没有任何技术内容与推荐系统、搜索或广告的架构、算法或应用相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14943v1": {
    "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
    "url": "https://www.alphaxiv.org/abs/2510.14943v1",
    "arxiv_id": "2510.14943v1",
    "authors": "Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:55:11",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
    "summary": "",
    "translation": "LaSeR：基于末令牌自奖励的强化学习",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及强化学习，但标题没有明确表明与推荐系统、搜索或广告领域的直接相关性。末令牌自奖励机制可能是一种通用的RL改进方法，但缺乏具体应用场景的说明，无法判断其在RecSys/Search/Ads领域的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14937v1": {
    "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World Clinical Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.14937v1",
    "arxiv_id": "2510.14937v1",
    "authors": "Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 17:50:04",
    "ori_summary": "Mental health disorders remain among the leading cause of disability worldwide, yet conditions such as depression, anxiety, and Post-Traumatic Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to subjective assessments, limited clinical resources, and stigma and low awareness. In primary care settings, studies show that providers misidentify depression or anxiety in over 60% of cases, highlighting the urgent need for scalable, accessible, and context-aware diagnostic tools that can support early detection and intervention. In this study, we evaluate the effectiveness of machine learning models for mental health screening using a unique dataset of 553 real-world, semistructured interviews, each paried with ground-truth diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We benchmark multiple model classes, including zero-shot prompting with GPT-4.1 Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic categories, with especially strongperformance on PTSD (up to 89% accuracy and 98% recall). We also find that using shorter context, focused context segments improves recall, suggesting that focused narrative cues enhance detection sensitivity. LoRA fine-tuning proves both efficient and effective, with lower-rank configurations (e.g., rank 8 and 16) maintaining competitive performance across evaluation metrics. Our results demonstrate that LLM-based models can offer substantial improvements over traditional self-report screening tools, providing a path toward low-barrier, AI-powerd early diagnosis. This work lays the groundwork for integrating machine learning into real-world clinical workflows, particularly in low-resource or high-stigma environments where access to timely mental health care is most limited.",
    "summary": "",
    "translation": "基于真实世界临床对话的AI驱动精神健康障碍早期诊断",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的AI应用，具体涉及精神健康诊断，这属于明确的无关主题（医学/生物学应用）。论文标题没有任何与推荐系统、搜索、广告或相关使能技术相关的元素，无法识别出任何潜在的跨领域应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14936v1": {
    "title": "Circuit Insights: Towards Interpretability Beyond Activations",
    "url": "https://www.alphaxiv.org/abs/2510.14936v1",
    "arxiv_id": "2510.14936v1",
    "authors": "Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 17:49:41",
    "ori_summary": "The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.",
    "summary": "",
    "translation": "电路洞见：超越激活的可解释性研究",
    "relevance_score": 2,
    "reasoning": "该论文关注模型可解释性技术，属于模型理解范畴而非核心推荐系统、搜索或广告的架构创新。虽然可解释性在部署中很重要，但该研究主要针对神经网络内部工作机制分析，缺乏明确的RecSys/Search/Ads应用场景，且不涉及Transformer效率提升、多模态建模或LLM在推荐中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14925v1": {
    "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14925v1",
    "arxiv_id": "2510.14925v1",
    "authors": "Akira Okutomi",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-16 17:40:28",
    "ori_summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.",
    "summary": "",
    "translation": "稳定但未校准：从滤波器到大型语言模型的过度自信之康德视角",
    "relevance_score": 1,
    "reasoning": "该论文主要探讨模型过度自信问题及其哲学视角，属于模型评估和校准范畴。虽然涉及LLMs，但焦点在于理论分析和哲学框架，而非核心推荐系统、搜索或广告的技术进步或应用，与当前关注的架构创新、效率提升或直接应用场景无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14922v1": {
    "title": "TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG",
    "url": "https://www.alphaxiv.org/abs/2510.14922v1",
    "arxiv_id": "2510.14922v1",
    "authors": "Annisaa Fitri Nurfidausi, Eleonora Mancini, Paolo Torroni",
    "categories": "cs.AI, cs.CL, cs.LG, eess.AS, eess.SP",
    "pub_date": "2025-10-16 17:39:59",
    "ori_summary": "Depression is a widespread mental health disorder, yet its automatic detection remains challenging. Prior work has explored unimodal and multimodal approaches, with multimodal systems showing promise by leveraging complementary signals. However, existing studies are limited in scope, lack systematic comparisons of features, and suffer from inconsistent evaluation protocols. We address these gaps by systematically exploring feature representations and modelling strategies across EEG, together with speech and text. We evaluate handcrafted features versus pre-trained embeddings, assess the effectiveness of different neural encoders, compare unimodal, bimodal, and trimodal configurations, and analyse fusion strategies with attention to the role of EEG. Consistent subject-independent splits are applied to ensure robust, reproducible benchmarking. Our results show that (i) the combination of EEG, speech and text modalities enhances multimodal detection, (ii) pretrained embeddings outperform handcrafted features, and (iii) carefully designed trimodal models achieve state-of-the-art performance. Our work lays the groundwork for future research in multimodal depression detection.",
    "summary": "",
    "translation": "TRI-DEP：基于语音、文本和脑电图的抑郁症检测三模态对比研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的抑郁症检测，使用语音、文本和EEG等生物医学数据。这与我的关注点完全无关，因为我的关注领域仅限于推荐系统、搜索和广告中的技术进展，不包括医疗应用或生物医学检测。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14919v1": {
    "title": "Predicting Task Performance with Context-aware Scaling Laws",
    "url": "https://www.alphaxiv.org/abs/2510.14919v1",
    "arxiv_id": "2510.14919v1",
    "authors": "Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:35:18",
    "ori_summary": "Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
    "summary": "研究传统扩展定律无法预测下游任务性能的问题，提出将下游性能建模为训练计算和上下文联合函数的可解释框架，揭示训练计算与上下文利用的相互作用机制。",
    "translation": "基于上下文感知的扩展定律预测任务性能",
    "relevance_score": 8,
    "reasoning": "该论文研究上下文感知的扩展定律，属于核心LLM技术进展（Enabling LLM Tech）。扩展定律对于预测不同规模模型在推荐、搜索系统中的性能至关重要，上下文感知特性可以更好地适应特定领域任务，为推荐系统、搜索广告中的模型规模选择和性能预测提供关键指导。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接研究LLM在下游任务中的性能预测，提出结合训练计算和上下文的新扩展定律框架，对搜索和推荐系统的LLM应用具有重要指导意义。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14915v1": {
    "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14915v1",
    "arxiv_id": "2510.14915v1",
    "authors": "Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 17:30:28",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.",
    "summary": "",
    "translation": "协调多样化模型：一种用于一致生成的逐层合并策略",
    "relevance_score": 2,
    "reasoning": "该论文关注模型合并策略以实现一致生成，这属于AIGC和内容生成领域，与我的关注点无关。虽然模型合并技术可能具有通用性，但论文标题明确指向生成任务，没有表明在推荐系统、搜索或广告中的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14913v1": {
    "title": "Budget-aware Test-time Scaling via Discriminative Verification",
    "url": "https://www.alphaxiv.org/abs/2510.14913v1",
    "arxiv_id": "2510.14913v1",
    "authors": "Kyle Montgomery, Sijun Tan, Yuqi Chen, Siyuan Zhuang, Tianjun Zhang, Raluca Ada Popa, Chenguang Wang",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-16 17:30:02",
    "ori_summary": "Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
    "summary": "",
    "translation": "基于判别性验证的预算感知测试时缩放",
    "relevance_score": 2,
    "reasoning": "该论文主要关注测试时缩放和预算约束下的模型验证，属于通用的机器学习效率优化范畴。虽然预算感知技术可能在资源受限的推荐或广告系统中具有间接应用价值，但论文标题没有明确指向推荐系统、搜索或广告领域的特定问题，也没有涉及LLM、Transformer架构或异构数据建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14901v1": {
    "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
    "url": "https://www.alphaxiv.org/abs/2510.14901v1",
    "arxiv_id": "2510.14901v1",
    "authors": "Aayush Karan, Yilun Du",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 17:18:11",
    "ori_summary": "Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.",
    "summary": "研究如何从预训练基础模型中激发推理能力，核心思想是使用基于MCMC的迭代采样算法，仅利用模型自身似然来提升推理性能，无需额外训练。",
    "translation": "基于采样的推理：您的基础模型比您想象的更智能",
    "relevance_score": 8,
    "reasoning": "该论文探讨推理采样方法，属于核心LLM技术进步范畴。在推荐系统和搜索中，更高效的推理采样技术可以显著提升模型在候选排序、多样性控制和冷启动问题上的性能表现，同时降低计算成本。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出无需额外训练即可从基础模型中激发推理能力的新方法，直接关联LLM核心技术进步，对搜索和推荐系统中的推理应用具有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14889v1": {
    "title": "Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media",
    "url": "https://www.alphaxiv.org/abs/2510.14889v1",
    "arxiv_id": "2510.14889v1",
    "authors": "Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Hari Sundaram, Koustuv Saha",
    "categories": "cs.SI, cs.AI, cs.CL, cs.CY, cs.HC",
    "pub_date": "2025-10-16 17:09:14",
    "ori_summary": "On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.",
    "summary": "",
    "translation": "基于社交媒体上的纵向和信息环境信号检测早期和隐性自杀意念",
    "relevance_score": 1,
    "reasoning": "该论文专注于心理健康检测和自杀预防，属于医疗/心理学领域应用。虽然涉及社交媒体数据，但核心关注的是临床诊断和干预，与推荐系统、搜索或广告的技术进步没有直接关联。论文内容不符合任何当前关注领域，包括核心推荐系统进展、LLM技术、Transformer架构改进或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14885v1": {
    "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
    "url": "https://www.alphaxiv.org/abs/2510.14885v1",
    "arxiv_id": "2510.14885v1",
    "authors": "Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 17:04:25",
    "ori_summary": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
    "summary": "",
    "translation": "畅所欲言：通过答案提取提升多模态大语言模型的细粒度视觉识别能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态大语言模型在细粒度视觉识别方面的改进，属于纯粹的视觉-语言模型能力提升研究。虽然提到了多模态建模，但其核心是视觉识别能力的增强，没有明确展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14871v1": {
    "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR",
    "url": "https://www.alphaxiv.org/abs/2510.14871v1",
    "arxiv_id": "2510.14871v1",
    "authors": "Erwei Wang, Samuel Bayliss, Andra Bisca, Zachary Blair, Sangeeta Chowdhary, Kristof Denolf, Jeff Fifield, Brandon Freiberger, Erika Hunhoff, Phil James-Roxby, Jack Lo, Joseph Melber, Stephen Neuendorffer, Eddie Richter, Andre Rosti, Javier Setoain, Gagandeep Singh, Endri Taka, Pranathi Vasireddy, Zhewen Yu, Niansong Zhang, Jinming Zhuang",
    "categories": "cs.CL, cs.AR, cs.LG",
    "pub_date": "2025-10-16 16:49:05",
    "ori_summary": "General-purpose compilers abstract away parallelism, locality, and synchronization, limiting their effectiveness on modern spatial architectures. As modern computing architectures increasingly rely on fine-grained control over data movement, execution order, and compute placement for performance, compiler infrastructure must provide explicit mechanisms for orchestrating compute and data to fully exploit such architectures. We introduce MLIR-AIR, a novel, open-source compiler stack built on MLIR that bridges the semantic gap between high-level workloads and fine-grained spatial architectures such as AMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured representations for asynchronous and hierarchical operations across compute and memory resources. AIR primitives allow the compiler to orchestrate spatial scheduling, distribute computation across hardware regions, and overlap communication with computation without relying on ad hoc runtime coordination or manual scheduling. We demonstrate MLIR-AIR's capabilities through two case studies: matrix multiplication and the multi-head attention block from the LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute efficiency and generates implementations with performance almost identical to state-of-the-art, hand-optimized matrix multiplication written using the lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we demonstrate that the AIR interface supports fused implementations using approximately 150 lines of code, enabling tractable expression of complex workloads with efficient mapping to spatial hardware. MLIR-AIR transforms high-level structured control flow into spatial programs that efficiently utilize the compute fabric and memory hierarchy of an NPU, leveraging asynchronous execution, tiling, and communication overlap through compiler-managed scheduling.",
    "summary": "",
    "translation": "从循环嵌套到硅芯片：使用MLIR-AIR将AI工作负载映射到AMD NPU",
    "relevance_score": 2,
    "reasoning": "该论文主要关注AI工作负载在AMD NPU上的硬件映射和编译优化，属于底层系统优化范畴。虽然LLM推理和推荐系统需要高效计算，但该工作聚焦于特定硬件平台的编译技术，与推荐系统、搜索或广告的核心算法进展及LLM技术应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14866v1": {
    "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14866v1",
    "arxiv_id": "2510.14866v1",
    "authors": "Hatef Otroshi Shahreza, Sébastien Marcel",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2025-10-16 16:42:27",
    "ori_summary": "Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.",
    "summary": "",
    "translation": "多模态大语言模型在人脸识别任务上的基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态LLM在人脸识别这一特定计算机视觉任务上的基准测试，这属于纯粹的视觉应用领域。虽然涉及多模态LLM技术，但人脸识别本身与推荐系统、搜索或广告的核心业务（如内容理解、用户建模、排序等）没有直接关联，且论文重点在于基准测试而非技术突破。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14865v1": {
    "title": "Midtraining Bridges Pretraining and Posttraining Distributions",
    "url": "https://www.alphaxiv.org/abs/2510.14865v1",
    "arxiv_id": "2510.14865v1",
    "authors": "Emmy Liu, Graham Neubig, Chenyan Xiong",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 16:39:52",
    "ori_summary": "Recently, many language models have been pretrained with a \"midtraining\" phase, in which higher quality, often instruction-formatted data, is mixed in at the end of pretraining. Despite the popularity of this practice, there is little scientific understanding of this phase of model training or why it is effective. In this work, we conduct the first systematic investigation of midtraining through controlled experiments with language models pretrained from scratch and fine-tuned on supervised finetuning datasets in different domains. We find that when compared after supervised fine-tuning, the effectiveness of midtraining is highest in the math and code domains, where midtraining can best reduce the syntactic gap between pretraining and posttraining data. In these cases, midtraining consistently outperforms continued pretraining in both in-domain validation loss as well as pretraining data forgetting after posttraining. We conduct ablations on the starting time of the midtraining phase and mixture weights of the midtraining data, using code midtraining as a case study, and find that timing has a greater impact than mixture weights, with earlier introduction of specialized data, yielding greater benefits in-domain as well as preserving general language modeling better. These findings establish midtraining as a domain adaptation technique that compared to continued pretraining yields better performance through reduced forgetting.",
    "summary": "论文研究语言模型训练中的midtraining阶段，核心发现是midtraining通过减少预训练与后训练数据之间的语法差距，作为领域适应技术优于持续预训练。",
    "translation": "中期训练桥接预训练与后训练分布",
    "relevance_score": 8,
    "reasoning": "该论文关注训练分布对齐的核心技术，属于'核心LLM进展'范畴。在推荐系统和搜索中，预训练模型与下游任务分布不匹配是常见问题，这种中期训练技术可以显著提升模型在特定领域（如电商推荐、搜索排序）的适应性和性能表现。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究LLM训练中的midtraining阶段，揭示其作为领域适应技术的机制，直接关联LLM核心训练技术进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14853v1": {
    "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
    "url": "https://www.alphaxiv.org/abs/2510.14853v1",
    "arxiv_id": "2510.14853v1",
    "authors": "Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 16:24:36",
    "ori_summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.",
    "summary": "研究MoE模型在部署中因分布偏移导致路由决策次优的问题；核心方法是通过数据无关的在线测试时框架，基于已生成序列进行自监督学习，动态优化专家选择而无需外部数据。",
    "translation": "动态重连专家：为混合专家模型中的在线自适应实现持续重路由以获得更好效果",
    "relevance_score": 8,
    "reasoning": "该论文涉及混合专家（MoE）模型的在线自适应和专家重路由，这直接属于'Enabling Transformer Tech'中的MoE架构效率优化。在推荐系统和搜索中，MoE模型可以更高效地处理大规模用户数据和上下文特征，动态专家重路由技术能够根据实时用户行为和数据分布变化进行模型调整，提升在线服务的性能和适应性。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对MoE架构的路由优化问题，属于Transformer架构效率提升的核心方向，提出的在线自适应路由方法对推荐系统和搜索中的动态环境具有直接应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14846v1": {
    "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14846v1",
    "arxiv_id": "2510.14846v1",
    "authors": "Zhuo-Yang Song",
    "categories": "cs.AI, cs.CL, cs.LO",
    "pub_date": "2025-10-16 16:18:37",
    "ori_summary": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.",
    "summary": "",
    "translation": "何处搜索：衡量LLM智能体的先验结构化搜索空间",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM智能体的搜索空间分析，属于LLM应用范畴，但与推荐系统、搜索或广告中的核心排名问题关联较弱。虽然可能涉及搜索策略优化，但缺乏明确的RecSys/Search/Ads应用场景，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14773v1": {
    "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14773v1",
    "arxiv_id": "2510.14773v1",
    "authors": "Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 15:09:22",
    "ori_summary": "Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt \"Answer:\". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.",
    "summary": "",
    "translation": "在思维中寻找答案：基于推理重新评估大型语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的评估和推理能力研究，属于纯粹的NLP评估基准范畴，与推荐系统、搜索或广告的核心技术无关。论文没有展示任何在推荐、搜索或广告领域的潜在应用价值，完全落在无关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14763v1": {
    "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
    "url": "https://www.alphaxiv.org/abs/2510.14763v1",
    "arxiv_id": "2510.14763v1",
    "authors": "Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 15:01:19",
    "ori_summary": "Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
    "summary": "",
    "translation": "COIG-Writer：一个带有思维过程的高质量中文创意写作数据集",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于中文创意写作数据集构建，属于内容生成领域。这与我的核心关注点（推荐系统、搜索、广告中的核心进展、使能技术及应用）完全无关，且明确排除了AIGC、内容生成等纯LLM中心化主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14756v1": {
    "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code",
    "url": "https://www.alphaxiv.org/abs/2510.14756v1",
    "arxiv_id": "2510.14756v1",
    "authors": "Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:57:01",
    "ori_summary": "Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\\%, delay efficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.",
    "summary": "",
    "translation": "Pluto：一个用于评估LLM生成硬件代码效率的基准",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM生成的硬件代码效率评估基准，属于LLM在硬件设计领域的应用。虽然涉及LLM技术，但与推荐系统、搜索或广告的核心领域进展、架构创新或直接应用没有明显关联，潜在相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14738v1": {
    "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14738v1",
    "arxiv_id": "2510.14738v1",
    "authors": "Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:40:02",
    "ori_summary": "Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",
    "summary": "",
    "translation": "AutoRubric-R1V：基于评分标准的生成式奖励用于忠实多模态推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态推理的忠实性评估和奖励机制，这属于纯粹的LLM评估和生成质量优化范畴。虽然提到了多模态，但缺乏与推荐系统、搜索或广告中异构数据处理的具体联系，更侧重于生成模型的可靠性而非实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14718v1": {
    "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms",
    "url": "https://www.alphaxiv.org/abs/2510.14718v1",
    "arxiv_id": "2510.14718v1",
    "authors": "Xingmeng Zhao, Dan Schumacher, Veronica Rammouz, Anthony Rios",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 14:18:31",
    "ori_summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.",
    "summary": "",
    "translation": "医疗AI中的推测模型风险：利用叙事方法揭示意外危害",
    "relevance_score": 1,
    "reasoning": "该论文聚焦医疗AI领域的模型风险、意外危害和伦理问题，这些都属于明确的无关主题。论文内容涉及医疗领域特定应用和伦理考量，与推荐系统、搜索、广告或相关使能技术没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14662v1": {
    "title": "Semantic Prosody in Machine Translation: the English-Chinese Case of Passive Structures",
    "url": "https://www.alphaxiv.org/abs/2510.14662v1",
    "arxiv_id": "2510.14662v1",
    "authors": "Xinyue Ma, Pol Pastells, Mireia Farrús, Mariona Taulé",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 13:16:59",
    "ori_summary": "Semantic prosody is a collocational meaning formed through the co-occurrence of a linguistic unit and a consistent series of collocates, which should be treated separately from semantic meaning. Since words that are literal translations of each other may have different semantic prosody, more attention should be paid to this linguistic property to generate accurate translations. However, current machine translation models cannot handle this problem. To bridge the gap, we propose an approach to teach machine translation models about semantic prosody of a specific structure. We focus on Chinese BEI passives and create a dataset of English-Chinese sentence pairs with the purpose of demonstrating the negative semantic prosody of BEI passives. Then we fine-tune OPUS-MT, NLLB-600M and mBART50 models with our dataset for the English-Chinese translation task. Our results show that fine-tuned MT models perform better on using BEI passives for translating unfavourable content and avoid using it for neutral and favourable content. Also, in NLLB-600M, which is a multilingual model, this knowledge of semantic prosody can be transferred from English-Chinese translation to other language pairs, such as Spanish-Chinese.",
    "summary": "",
    "translation": "机器翻译中的语义韵律：英语-中文被动结构的案例研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译中特定语言结构（被动语态）的语义韵律问题，属于纯机器翻译领域的技术细节。虽然涉及语言模型应用，但内容局限于翻译质量评估和语言结构分析，与推荐系统、搜索或广告的核心技术进展没有直接关联，也没有展示出在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14628v1": {
    "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
    "url": "https://www.alphaxiv.org/abs/2510.14628v1",
    "arxiv_id": "2510.14628v1",
    "authors": "Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:40:37",
    "ori_summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
    "summary": "",
    "translation": "RLAIF-SPA：通过强化学习从AI反馈优化基于大语言模型的情感语音合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成的情感优化，属于语音处理领域，与推荐系统、搜索或广告的核心技术无关。强化学习从AI反馈(RLAIF)的应用仅限于语音生成，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14621v1": {
    "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.14621v1",
    "arxiv_id": "2510.14621v1",
    "authors": "Yuanyi Song, Heyuan Huang, Qiqiang Lin, Yin Zhao, Xiangmou Qu, Jun Wang, Xingyu Lou, Weiwen Liu, Zhuosheng Zhang, Jun Wang, Yong Yu, Weinan Zhang, Zhaoxiang Wang",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-16 12:30:05",
    "ori_summary": "The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined \"golden path\", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.",
    "summary": "",
    "translation": "ColorBench：基于图结构框架的移动智能体复杂长周期任务基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注移动智能体的基准测试和长周期任务评估，属于机器人学和智能体研究领域。虽然提到了图结构框架，但其核心焦点是移动智能体的性能评估，与推荐系统、搜索或广告的核心技术进展没有直接关联。该工作缺乏在RecSys/Search/Ads领域的明确应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14620v1": {
    "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14620v1",
    "arxiv_id": "2510.14620v1",
    "authors": "Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:29:40",
    "ori_summary": "Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \\textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \\textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.",
    "summary": "",
    "translation": "代码驱动的数字序列计算：增强大型语言模型的归纳推理能力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注通过代码驱动方法增强LLM的归纳推理能力，属于核心LLM技术进展。虽然推理能力对推荐和搜索有一定潜在价值（如模式识别、序列预测），但论文聚焦于数字序列计算这一特定任务，与推荐系统、搜索广告的直接应用关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14616v1": {
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures",
    "url": "https://www.alphaxiv.org/abs/2510.14616v1",
    "arxiv_id": "2510.14616v1",
    "authors": "Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 12:23:13",
    "ori_summary": "Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
    "summary": "",
    "translation": "超越正确性：评估跨文化主观写作偏好",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于跨文化的主观写作偏好评估，这属于纯NLP评估基准范畴，与推荐系统、搜索或广告的核心技术进展无关。论文内容涉及文化差异下的主观偏好测量，没有展示在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14591v1": {
    "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.14591v1",
    "arxiv_id": "2510.14591v1",
    "authors": "Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein",
    "categories": "cs.HC, cs.AI, cs.CL",
    "pub_date": "2025-10-16 11:53:17",
    "ori_summary": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
    "summary": "",
    "translation": "即时目标：一种用于专用AI交互的通用方法",
    "relevance_score": 3,
    "reasoning": "该论文标题暗示了一种用于专门AI交互的通用方法，可能涉及目标设定或优化技术。虽然通用AI交互方法在理论上可能应用于推荐系统或搜索中的用户交互优化，但标题过于宽泛，没有明确指向推荐、搜索或广告领域的具体技术，也没有明确涉及LLM、Transformer架构或异构数据建模等焦点领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14583v1": {
    "title": "Talking Points: Describing and Localizing Pixels",
    "url": "https://www.alphaxiv.org/abs/2510.14583v1",
    "arxiv_id": "2510.14583v1",
    "authors": "Matan Rusanovsky, Shimon Malnick, Shai Avidan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-16 11:42:03",
    "ori_summary": "Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at https://github.com/matanr/Talking_Points.",
    "summary": "",
    "translation": "对话要点：像素描述与定位",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉中的像素级描述和定位任务，属于纯粹的视觉技术范畴。虽然标题中的'描述'可能与语言模型相关，但论文核心聚焦于视觉像素处理，缺乏与推荐系统、搜索或广告领域的明确关联或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14565v1": {
    "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14565v1",
    "arxiv_id": "2510.14565v1",
    "authors": "Kyubyung Chae, Gihoon Kim, Gyuseong Lee, Taesup Kim, Jaejin Lee, Heejin Kim",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 11:17:44",
    "ori_summary": "Recent trends in LLMs development clearly show growing interest in the use and application of sovereign LLMs. The global debate over sovereign LLMs highlights the need for governments to develop their LLMs, tailored to their unique socio-cultural and historical contexts. However, there remains a shortage of frameworks and datasets to verify two critical questions: (1) how well these models align with users' socio-cultural backgrounds, and (2) whether they maintain safety and technical robustness without exposing users to potential harms and risks. To address this gap, we construct a new dataset and introduce an analytic framework for extracting and evaluating the socio-cultural elements of sovereign LLMs, alongside assessments of their technical robustness. Our experimental results demonstrate that while sovereign LLMs play a meaningful role in supporting low-resource languages, they do not always meet the popular claim that these models serve their target users well. We also show that pursuing this untested claim may lead to underestimating critical quality attributes such as safety. Our study suggests that advancing sovereign LLMs requires a more extensive evaluation that incorporates a broader range of well-grounded and practical criteria.",
    "summary": "",
    "translation": "评估主权大语言模型的社会文化对齐与技术安全性",
    "relevance_score": 1,
    "reasoning": "该论文专注于LLM的安全性和社会文化对齐评估，这属于伦理、公平性等非技术性话题，与用户关注的推荐系统、搜索、广告核心技术进展及LLM架构效率等焦点完全无关。论文内容不涉及任何在推荐、搜索或广告领域的潜在技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14509v1": {
    "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task",
    "url": "https://www.alphaxiv.org/abs/2510.14509v1",
    "arxiv_id": "2510.14509v1",
    "authors": "Jingyao Liu, Chen Huang, Zhizhao Guan, Wenqiang Lei, Yang Deng",
    "categories": "cs.SE, cs.AI, cs.CL",
    "pub_date": "2025-10-16 09:54:26",
    "ori_summary": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple BDD test scenarios with corresponding Python step implementations for each requirement}, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with E2EDev}, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.",
    "summary": "",
    "translation": "E2Edev：在端到端软件开发任务中评估大型语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM在软件开发任务中的基准测试，这属于纯粹的LLM应用评估范畴。虽然涉及LLM技术，但其应用领域（软件开发）与推荐系统、搜索或广告没有直接关联，也不涉及这些领域的核心进展或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14504v1": {
    "title": "Efficient Seq2seq Coreference Resolution Using Entity Representations",
    "url": "https://www.alphaxiv.org/abs/2510.14504v1",
    "arxiv_id": "2510.14504v1",
    "authors": "Matt Grenander, Shay B. Cohen, Mark Steedman",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 09:50:03",
    "ori_summary": "Seq2seq coreference models have introduced a new paradigm for coreference resolution by learning to generate text corresponding to coreference labels, without requiring task-specific parameters. While these models achieve new state-of-the-art performance, they do so at the cost of flexibility and efficiency. In particular, they do not efficiently handle incremental settings such as dialogue, where text must processed sequentially. We propose a compressed representation in order to improve the efficiency of these methods in incremental settings. Our method works by extracting and re-organizing entity-level tokens, and discarding the majority of other input tokens. On OntoNotes, our best model achieves just 0.6 CoNLL F1 points below a full-prefix, incremental baseline while achieving a compression ratio of 1.8. On LitBank, where singleton mentions are annotated, it passes state-of-the-art performance. Our results indicate that discarding a wide portion of tokens in seq2seq resolvers is a feasible strategy for incremental coreference resolution.",
    "summary": "",
    "translation": "使用实体表示的高效序列到序列共指消解",
    "relevance_score": 2,
    "reasoning": "该论文专注于NLP领域的共指消解任务，属于纯粹的NLP技术研究。虽然序列到序列架构和实体表示在推荐系统中可能有间接应用，但论文本身没有明确展示与推荐系统、搜索或广告的直接关联，也没有涉及Transformer架构创新或跨模态建模等核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14466v1": {
    "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14466v1",
    "arxiv_id": "2510.14466v1",
    "authors": "Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, Biqing Huang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 09:08:24",
    "ori_summary": "As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.",
    "summary": "",
    "translation": "LiRA：跨语言大语言模型的语言鲁棒锚定",
    "relevance_score": 3,
    "reasoning": "该论文专注于跨语言LLM的语言鲁棒性，属于核心LLM技术进展。虽然多语言能力对国际化的搜索和推荐系统有潜在应用价值，但论文标题未明确指向RecSys/Search/Ads的具体应用场景，且可能更偏向NLP领域的一般性语言处理问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14453v1": {
    "title": "Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14453v1",
    "arxiv_id": "2510.14453v1",
    "authors": "Reid T. Johnson, Michelle D. Pain, Jordan D. West",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 08:52:52",
    "ori_summary": "We present Natural Language Tools (NLT), a framework that replaces programmatic JSON tool calling in large language models (LLMs) with natural language outputs. By decoupling tool selection from response generation, NLT eliminates task interference and format constraints that degrade tool call performance. When evaluated across 10 models and 6,400 trials spanning customer service and mental health domains, NLT improves tool calling accuracy by 18.4 percentage points while reducing output variance by 70%. Open-weight models see the largest gains, surpassing flagship closed-weight alternatives, with implications for model training in both reinforcement learning and supervised fine-tuning stages. These improvements persist under prompt perturbations and extend tool-calling capabilities to models lacking native support.",
    "summary": "",
    "translation": "自然语言工具：大语言智能体中工具调用的自然语言方法",
    "relevance_score": 8,
    "reasoning": "该论文提出通过自然语言进行工具调用的方法，属于核心LLM技术的进展。在推荐系统和搜索领域，这种方法可以显著增强LLM与外部系统（如数据库、推荐引擎、搜索索引）的交互能力，实现更复杂的任务执行和实时信息获取，从而提升推荐和搜索的准确性和实用性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14438v1": {
    "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
    "url": "https://www.alphaxiv.org/abs/2510.14438v1",
    "arxiv_id": "2510.14438v1",
    "authors": "Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 08:37:42",
    "ori_summary": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
    "summary": "",
    "translation": "探索以进化：通过主动在线探索扩展深度研究智能体的进化聚合逻辑",
    "relevance_score": 2,
    "reasoning": "该论文主要关注深度研究智能体的在线探索和进化聚合逻辑，属于强化学习和智能体研究的范畴。虽然提到了在线探索和进化逻辑，但没有明确展示与推荐系统、搜索或广告的直接关联，也没有涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14420v1": {
    "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following",
    "url": "https://www.alphaxiv.org/abs/2510.14420v1",
    "arxiv_id": "2510.14420v1",
    "authors": "Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 08:24:44",
    "ori_summary": "Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if",
    "summary": "",
    "translation": "指令即所需：用于指令跟随的自监督强化学习",
    "relevance_score": 3,
    "reasoning": "该论文主要关注自监督强化学习在指令跟随任务中的应用，这属于强化学习的范畴。虽然指令跟随在对话系统中可能有应用，但论文没有明确展示与推荐系统、搜索或广告的直接相关性。对于'使能技术'类别，其潜在应用在推荐/搜索/广告领域不够明确或直接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14406v1": {
    "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
    "url": "https://www.alphaxiv.org/abs/2510.14406v1",
    "arxiv_id": "2510.14406v1",
    "authors": "Xikai Zhang, Bo Wang, Likang Xiao, Yongzhi Li, Quan Chen, Wenju Wu, Liu Liu",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2025-10-16 08:06:35",
    "ori_summary": "Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
    "summary": "",
    "translation": "IMAGINE：将多智能体系统集成到单一模型中，用于复杂推理与规划",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多智能体系统在复杂推理与规划中的应用，这属于通用AI推理能力范畴。虽然多智能体协作思想可能启发推荐系统中多策略协同优化（如多目标排序），但论文标题未明确指向推荐、搜索或广告领域的具体应用场景，关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14398v1": {
    "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14398v1",
    "arxiv_id": "2510.14398v1",
    "authors": "Shiyao Ding, Takayuki Ito",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 07:54:02",
    "ori_summary": "Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of \"Your Next Token Prediction (YNTP)\", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100",
    "summary": "",
    "translation": "您的下一个令牌预测：面向个性化响应生成的多语言基准",
    "relevance_score": 8,
    "reasoning": "该论文聚焦于个性化响应生成，这直接适用于推荐系统和搜索中的个性化内容生成。多语言基准测试有助于开发更强大的个性化模型，可应用于跨语言推荐和搜索场景。令牌预测技术是LLM的核心能力，在个性化推荐和搜索排序中具有直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14395v1": {
    "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.14395v1",
    "arxiv_id": "2510.14395v1",
    "authors": "Jun Li, Qun Zhao",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 07:47:03",
    "ori_summary": "Suicide remains a critical global public health issue. While previous studies have provided valuable insights into detecting suicidal expressions in individual social media posts, limited attention has been paid to the analysis of longitudinal, sequential comment trees for predicting a user's evolving suicidal risk. Users, however, often reveal their intentions through historical posts and interactive comments over time. This study addresses this gap by investigating how the information in comment trees affects both the discrimination and prediction of users' suicidal risk levels. We constructed a high-quality annotated dataset, sourced from Reddit, which incorporates users' posting history and comments, using a refined four-label annotation framework based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical analysis of the dataset, along with experimental results from Large Language Models (LLMs) experiments, demonstrates that incorporating comment trees data significantly enhances the discrimination and prediction of user suicidal risk levels. This research offers a novel insight to enhancing the detection accuracy of at-risk individuals, thereby providing a valuable foundation for early suicide intervention strategies.",
    "summary": "",
    "translation": "自杀评论树数据集：通过上下文分析增强风险评估与预测",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于自杀风险评估和心理健康领域，这属于医疗/心理学应用范畴，与推荐系统、搜索或广告的核心技术领域完全无关。论文内容涉及风险预测和上下文分析，但这些技术应用在医疗健康场景中，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14381v1": {
    "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers",
    "url": "https://www.alphaxiv.org/abs/2510.14381v1",
    "arxiv_id": "2510.14381v1",
    "authors": "Andrew Zhao, Reshmi Ghosh, Vitor Carvalho, Emily Lawton, Keegan Hines, Gao Huang, Jack W. Stokes",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CR",
    "pub_date": "2025-10-16 07:28:54",
    "ori_summary": "Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.",
    "summary": "",
    "translation": "我的优化提示词是否已遭泄露？探索基于大语言模型的优化器漏洞",
    "relevance_score": 2,
    "reasoning": "该论文主要关注LLM优化器的安全漏洞和提示词泄露问题，这属于安全性和隐私保护范畴，属于明确的无关主题。虽然涉及LLM技术，但核心关注点与推荐系统、搜索或广告的技术进步无关，且安全漏洞研究不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14369v1": {
    "title": "From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program",
    "url": "https://www.alphaxiv.org/abs/2510.14369v1",
    "arxiv_id": "2510.14369v1",
    "authors": "Joseph E. Trujillo-Falcon, Monica L. Bozeman, Liam E. Llewellyn, Samuel T. Halvorson, Meryl Mizell, Stuti Deshpande, Bob Manning, Todd Fagin",
    "categories": "cs.CL, cs.AI, cs.CY, cs.HC",
    "pub_date": "2025-10-16 07:06:05",
    "ori_summary": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.",
    "summary": "",
    "translation": "从二元到双语：美国国家气象局如何利用人工智能开发综合性翻译项目",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于气象领域的翻译应用，属于特定领域（气象服务）的AI应用，与推荐系统、搜索或广告的核心技术无关。论文内容涉及翻译程序开发，属于纯粹的NLP应用，没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14365v1": {
    "title": "On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?",
    "url": "https://www.alphaxiv.org/abs/2510.14365v1",
    "arxiv_id": "2510.14365v1",
    "authors": "Anyun Zhuo, Xuefei Ning, Ningyuan Li, Yu Wang, Pinyan Lu",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 06:59:58",
    "ori_summary": "This work investigates the resilience of contemporary LLMs against frequent and structured character-level perturbations, specifically through the insertion of noisy characters after each input character. We introduce \\nameshort{}, a practical method that inserts invisible Unicode control characters into text to discourage LLM misuse in scenarios such as online exam systems. Surprisingly, despite strong obfuscation that fragments tokenization and reduces the signal-to-noise ratio significantly, many LLMs still maintain notable performance. Through comprehensive evaluation across model-, problem-, and noise-related configurations, we examine the extent and mechanisms of this robustness, exploring both the handling of character-level tokenization and \\textit{implicit} versus \\textit{explicit} denoising mechanism hypotheses of character-level noises. We hope our findings on the low-level robustness of LLMs will shed light on the risks of their misuse and on the reliability of deploying LLMs across diverse applications.",
    "summary": "",
    "translation": "论大语言模型处理字符级扰动的能力：效果如何及机制探究",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLMs对字符级扰动的鲁棒性，这属于LLM评估和基准测试范畴，与纯粹的NLP评估主题相关。虽然LLM鲁棒性在理论上可能对搜索系统中的查询理解有间接影响，但该论文没有明确展示在推荐系统、搜索或广告中的直接应用潜力，且更侧重于NLP中心的评估问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14359v1": {
    "title": "AI for Service: Proactive Assistance with AI Glasses",
    "url": "https://www.alphaxiv.org/abs/2510.14359v1",
    "arxiv_id": "2510.14359v1",
    "authors": "Zichen Wen, Yiyu Wang, Chenfei Liao, Boxue Yang, Junxian Li, Weifeng Liu, Haocong He, Bolong Feng, Xuyang Liu, Yuanhuiyi Lyu, Xu Zheng, Xuming Hu, Linfeng Zhang",
    "categories": "cs.AI, cs.CL, cs.CV",
    "pub_date": "2025-10-16 06:55:28",
    "ori_summary": "In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.",
    "summary": "",
    "translation": "AI服务：通过AI眼镜实现主动式辅助",
    "relevance_score": 2,
    "reasoning": "该论文主要关注AI眼镜的主动辅助应用，属于硬件交互和用户界面领域，与推荐系统、搜索或广告的核心技术进展缺乏直接关联。虽然涉及用户交互，但未明确涉及LLM技术、Transformer架构或推荐系统的核心算法，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14353v1": {
    "title": "CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.14353v1",
    "arxiv_id": "2510.14353v1",
    "authors": "Ziad Elshaer, Essam A. Rashed",
    "categories": "cs.CL, cs.AI, physics.med-ph",
    "pub_date": "2025-10-16 06:46:11",
    "ori_summary": "High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0\\%) and MedMCQA (78.0\\%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings.",
    "summary": "",
    "translation": "CURE：基于置信度驱动的统一推理集成框架用于医疗问答",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的问答应用，属于明确的医学领域特定应用，这在无关主题中被明确排除。虽然提到了集成框架和置信度驱动方法，但这些技术元素被限定在医疗问答这一无关领域内，与推荐系统、搜索或广告的核心技术焦点没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14351v1": {
    "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts",
    "url": "https://www.alphaxiv.org/abs/2510.14351v1",
    "arxiv_id": "2510.14351v1",
    "authors": "Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 06:39:27",
    "ori_summary": "Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
    "summary": "",
    "translation": "超越单一世界：跨多元宇宙情境下角色扮演中超级英雄的基准测试",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于角色扮演游戏中的多元宇宙基准测试，属于游戏AI和角色扮演评估领域。这与推荐系统、搜索或广告的核心技术进展、LLM基础技术、Transformer架构改进或异构数据统一建模均无直接关联，且未显示出在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14332v1": {
    "title": "A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease",
    "url": "https://www.alphaxiv.org/abs/2510.14332v1",
    "arxiv_id": "2510.14332v1",
    "authors": "Yangyang Li",
    "categories": "cs.CL, cs.AI, cs.LG, eess.AS, I.2.7; I.2.6",
    "pub_date": "2025-10-16 06:10:31",
    "ori_summary": "Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD patients, leading to early treatments that lessen symptoms and alleviating financial burden of health care. As one of the leading signs of AD, language capability changes can be used for early diagnosis of AD. In this paper, I develop a robust classification method using hybrid word embedding and fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early detection of AD. Specifically, we create a hybrid word embedding based on word vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The scores identify whether a sentence is fluent or not and capture semantic context of the sentences. I enrich the word embedding by adding linguistic features to analyze syntax and semantics. Further, we input an embedded feature vector into logistic regression and fine tune hyperparameters throughout the pipeline. By tuning hyperparameters of the machine learning pipeline (e.g., model regularization parameter, learning rate and vector size of Doc2Vec, and vector size of ELMo), I achieve 91% classification accuracy and an Area Under the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the model stability through repeated experiments and find that the model is stable even though the training data is split randomly (standard deviation of accuracy = 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method is accurate and stable. This model can be used as a large-scale screening method for AD, as well as a complementary examination for doctors to detect AD.",
    "summary": "",
    "translation": "基于混合词嵌入的阿尔茨海默病早期诊断鲁棒分类方法",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的阿尔茨海默病诊断应用，属于明确的医疗领域特定应用。论文虽然涉及词嵌入技术，但应用场景完全在医疗诊断领域，与推荐系统、搜索或广告没有任何关联，也不涉及核心LLM技术或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14318v1": {
    "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL",
    "url": "https://www.alphaxiv.org/abs/2510.14318v1",
    "arxiv_id": "2510.14318v1",
    "authors": "Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 05:29:36",
    "ori_summary": "Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.",
    "summary": "",
    "translation": "基于多轮强化学习评估与减少语言模型的欺骗性对话",
    "relevance_score": 2,
    "reasoning": "该论文主要关注语言模型的欺骗性对话评估与减少，这属于纯粹的NLP评估和安全性问题，与推荐系统、搜索或广告的核心技术无关。虽然涉及强化学习，但应用场景是对话安全性而非推荐/搜索/广告中的排序或建模问题，因此相关性很低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14312v1": {
    "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
    "url": "https://www.alphaxiv.org/abs/2510.14312v1",
    "arxiv_id": "2510.14312v1",
    "authors": "Mason Nakamura, Abhinav Kumar, Saaduddin Mahmud, Sahar Abdelnabi, Shlomo Zilberstein, Eugene Bagdasarian",
    "categories": "cs.AI, cs.CL, cs.CR, I.2.7; I.2.11",
    "pub_date": "2025-10-16 05:19:13",
    "ori_summary": "A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.",
    "summary": "",
    "translation": "Terrarium：重新审视多智能体安全、隐私与安全研究的黑板架构",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于多智能体系统的安全、隐私和安全研究，这些主题在无关主题列表中明确被排除。虽然提到了'黑板架构'这一多智能体协作模式，但论文的核心关注点完全落在安全、隐私和安全领域，与推荐系统、搜索或广告的核心技术进展没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14307v1": {
    "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking",
    "url": "https://www.alphaxiv.org/abs/2510.14307v1",
    "arxiv_id": "2510.14307v1",
    "authors": "Sathyanarayanan Ramamoorthy, Vishwa Shah, Simran Khanuja, Zaid Sheikh, Shan Jie, Ann Chia, Shearman Chua, Graham Neubig",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 05:06:54",
    "ori_summary": "This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin",
    "summary": "",
    "translation": "MERLIN：多语言多模态实体识别与链接的测试平台",
    "relevance_score": 3,
    "reasoning": "该论文聚焦多语言多模态实体识别与链接，虽涉及实体识别这一搜索系统的基础技术，但其多语言和多模态特性更偏向通用NLP任务，与推荐系统、搜索或广告的核心进展（如排序模型、用户行为建模）关联较弱。多模态实体识别在搜索中可用于增强查询理解，但论文作为测试平台而非方法创新，实际应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14305v1": {
    "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14305v1",
    "arxiv_id": "2510.14305v1",
    "authors": "Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 04:59:52",
    "ori_summary": "Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs' ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: https://github.com/mahbubhimel/MathMist",
    "summary": "",
    "translation": "MathMist：一个用于数学问题求解与推理的并行多语言基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于数学问题求解的基准数据集构建，属于纯粹的评估基准工作。虽然数学推理是LLM的一个能力维度，但该论文没有涉及任何与推荐系统、搜索或广告相关的技术应用或架构创新，完全属于被排除的'评估基准'范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14303v1": {
    "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers",
    "url": "https://www.alphaxiv.org/abs/2510.14303v1",
    "arxiv_id": "2510.14303v1",
    "authors": "Ziye Xia, Sergei S. Ospichev",
    "categories": "cs.CL, cs.LG, I.2.7",
    "pub_date": "2025-10-16 04:58:28",
    "ori_summary": "In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.",
    "summary": "",
    "translation": "基于智能体和OpenAlex知识图谱的约束驱动小型语言模型：在学术论文中挖掘概念路径与发现创新点",
    "relevance_score": 2,
    "reasoning": "该论文主要关注学术论文中的概念路径挖掘和创新点发现，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然涉及知识图谱和小型语言模型，但其应用场景（学术论文分析）与我的关注领域（RecSys/Search/Ads）相关性较弱，且未明确展示这些技术在推荐或搜索系统中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14276v1": {
    "title": "Qwen3Guard Technical Report",
    "url": "https://www.alphaxiv.org/abs/2510.14276v1",
    "arxiv_id": "2510.14276v1",
    "authors": "Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, Baosong Yang, Chen Cheng, Jialong Tang, Jiandong Jiang, Jianwei Zhang, Jijie Xu, Ming Yan, Minmin Sun, Pei Zhang, Pengjun Xie, Qiaoyu Tang, Qin Zhu, Rong Zhang, Shibin Wu, Shuo Zhang, Tao He, Tianyi Tang, Tingyu Xia, Wei Liao, Weizhou Shen, Wenbiao Yin, Wenmeng Zhou, Wenyuan Yu, Xiaobin Wang, Xiaodong Deng, Xiaodong Xu, Xinyu Zhang, Yang Liu, Yeqiu Li, Yi Zhang, Yong Jiang, Yu Wan, Yuxin Zhou",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 04:00:18",
    "ori_summary": "As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
    "summary": "",
    "translation": "Qwen3Guard技术报告",
    "relevance_score": 1,
    "reasoning": "这个标题表明这是一个技术报告，可能涉及模型安全或防护相关内容，属于被明确排除的隐私、安全、公平性等非技术性主题。没有迹象表明该论文涉及推荐系统、搜索、广告的核心进展或LLM/Transformer架构的技术创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14274v1": {
    "title": "Retrofitting Small Multilingual Models for Retrieval: Matching 7B Performance with 300M Parameters",
    "url": "https://www.alphaxiv.org/abs/2510.14274v1",
    "arxiv_id": "2510.14274v1",
    "authors": "Lifu Tu, Yingbo Zhou, Semih Yavuz",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:48:59",
    "ori_summary": "Training effective multilingual embedding models presents unique challenges due to the diversity of languages and task objectives. Although small multilingual models (<1 B parameters) perform well on multilingual tasks generally, they consistently lag behind larger models (>1 B) in the most prevalent use case: retrieval. This raises a critical question: Can smaller models be retrofitted specifically for retrieval tasks to enhance their performance? In this work, we investigate key factors that influence the effectiveness of multilingual embeddings, focusing on training data scale, negative sampling strategies, and data diversity. We find that while increasing the scale of training data yields initial performance gains, these improvements quickly plateau - indicating diminishing returns. Incorporating hard negatives proves essential for consistently improving retrieval accuracy. Furthermore, our analysis reveals that task diversity in the training data contributes more significantly to performance than language diversity alone. As a result, we develop a compact (approximately 300M) multilingual model that achieves retrieval performance comparable to or even surpassing current strong 7B models.",
    "summary": "",
    "translation": "改造小型多语言模型用于检索：以3亿参数匹配70亿参数模型性能",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'范畴，专注于高效检索模型的开发，这对于搜索和推荐系统至关重要。通过参数效率优化实现性能匹配，这种方法可以直接应用于大规模搜索和推荐系统的部署，降低计算成本同时保持检索质量。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14271v1": {
    "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14271v1",
    "arxiv_id": "2510.14271v1",
    "authors": "Yilun Zheng, Dan Yang, Jie Li, Lin Shang, Lihui Chen, Jiahao Xu, Sitao Luan",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-16 03:41:44",
    "ori_summary": "Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.",
    "summary": "",
    "translation": "少即是多：为检索增强生成去噪知识图谱",
    "relevance_score": 4,
    "reasoning": "该论文主要关注知识图谱去噪以改进检索增强生成（RAG），这属于LLM应用范畴。虽然RAG在搜索系统中具有潜在应用价值，但论文焦点更偏向知识图谱处理和通用RAG改进，而非专门针对推荐、搜索或广告系统的核心算法或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14262v1": {
    "title": "CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions",
    "url": "https://www.alphaxiv.org/abs/2510.14262v1",
    "arxiv_id": "2510.14262v1",
    "authors": "Zihao Fu, Ming Liao, Chris Russell, Zhenguang G. Cai",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 03:27:15",
    "ori_summary": "Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.",
    "summary": "",
    "translation": "CAST：基于谱追踪的组合分析用于理解Transformer层功能",
    "relevance_score": 8,
    "reasoning": "This paper focuses on analyzing Transformer layer functions through spectral tracking, which directly falls under 'Enabling Transformer Tech' for understanding and potentially improving Transformer architectures. The insights from understanding layer functions could enable more efficient Transformer designs and better layer composition strategies, directly applicable to improving model efficiency in large-scale recommendation and search systems.",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14261v1": {
    "title": "Rewriting History: A Recipe for Interventional Analyses to Study Data Effects on Model Behavior",
    "url": "https://www.alphaxiv.org/abs/2510.14261v1",
    "arxiv_id": "2510.14261v1",
    "authors": "Rahul Nadkarni, Yanai Elazar, Hila Gonen, Noah A. Smith",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:22:48",
    "ori_summary": "We present an experimental recipe for studying the relationship between training data and language model (LM) behavior. We outline steps for intervening on data batches -- i.e., ``rewriting history'' -- and then retraining model checkpoints over that data to test hypotheses relating data to behavior. Our recipe breaks down such an intervention into stages that include selecting evaluation items from a benchmark that measures model behavior, matching relevant documents to those items, and modifying those documents before retraining and measuring the effects. We demonstrate the utility of our recipe through case studies on factual knowledge acquisition in LMs, using both cooccurrence statistics and information retrieval methods to identify documents that might contribute to knowledge learning. Our results supplement past observational analyses that link cooccurrence to model behavior, while demonstrating that extant methods for identifying relevant training documents do not fully explain an LM's ability to correctly answer knowledge questions. Overall, we outline a recipe that researchers can follow to test further hypotheses about how training data affects model behavior. Our code is made publicly available to promote future work.",
    "summary": "",
    "translation": "重写历史：研究数据对模型行为影响的干预分析方案",
    "relevance_score": 3,
    "reasoning": "该论文关注数据对模型行为影响的干预分析，这属于模型可解释性和因果分析范畴。虽然数据影响分析在推荐/搜索系统中具有潜在价值（如理解训练数据偏差如何影响推荐结果），但论文标题未明确指向推荐系统、搜索或广告的具体应用场景，也未涉及LLM、Transformer架构或异质数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14252v1": {
    "title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems",
    "url": "https://www.alphaxiv.org/abs/2510.14252v1",
    "arxiv_id": "2510.14252v1",
    "authors": "Jihao Zhao, Zhiyuan Ji, Simin Niu, Hanyu Wang, Feiyu Xiong, Zhiyu Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 03:09:51",
    "ori_summary": "The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
    "summary": "该论文研究传统RAG系统中文本块处理的局限性问题，核心思想是将被动分块转变为主动理解，通过模拟人类认知过程构建场景感知的文档记忆，利用LLM生成逻辑大纲指导结构化分块和多路径采样机制提取最优文档记忆。",
    "translation": "MoM：面向检索增强生成系统的场景感知文档记忆混合模型",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及检索增强生成（RAG）系统，这是搜索和推荐系统中的关键技术。场景感知文档记忆混合的创新可以显著提升个性化搜索和推荐的质量，通过更好地理解用户上下文和场景来改进文档检索和内容生成，属于直接LLM应用和核心领域进展的交叉范畴。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出的主动文档记忆提取框架直接改进了RAG系统，与搜索和推荐系统的核心需求高度相关，其多场景记忆混合机制对处理异构数据具有重要启发价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2510.14242v1": {
    "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.14242v1",
    "arxiv_id": "2510.14242v1",
    "authors": "Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-16 02:54:01",
    "ori_summary": "Large Language Models (LLMs) often produce inconsistent answers when faced with different phrasings of the same prompt. In this paper, we propose Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves robustness to such perturbations. $F^2C$ is composed of two key components. The first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt variations to create a hard pseudo-label. The second is a representation alignment loss that pulls lower-confidence and non-majority predictors toward the consensus established by high-confidence, majority-voting variations. We evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%, improves mean $F_1$ by 8.94%, and reduces performance variance across formats by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively, increasing $\\overline{F_1}$ and agreement while decreasing variance across most source-target pairs. Finally, when trained on only a subset of prompt perturbations and evaluated on held-out formats, $F^2C$ consistently improves both performance and agreement while reducing variance. These findings highlight $F^2C$ as an effective unsupervised method for enhancing LLM consistency, performance, and generalization under prompt perturbations. Code is available at https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.",
    "summary": "",
    "translation": "翻转一致性：针对大语言模型中提示扰动鲁棒性的无监督训练",
    "relevance_score": 7,
    "reasoning": "该论文专注于提升LLMs对提示扰动的鲁棒性，这属于核心LLM技术的进步。在推荐系统和搜索应用中，用户查询通常存在表述变化和噪声，这种鲁棒性训练方法可以显著提升模型对多样化用户输入的理解能力，从而提高推荐和搜索结果的稳定性与准确性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14232v1": {
    "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models",
    "url": "https://www.alphaxiv.org/abs/2510.14232v1",
    "arxiv_id": "2510.14232v1",
    "authors": "Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 02:19:25",
    "ori_summary": "Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \\gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.",
    "summary": "",
    "translation": "通过扩展测试时计算，使用开源权重模型实现国际信息学奥林匹克竞赛金牌",
    "relevance_score": 1,
    "reasoning": "该论文主要关注测试时计算扩展和竞赛性能优化，这与推荐系统、搜索或广告的核心技术需求没有直接关联。虽然涉及模型性能提升，但特定于竞赛场景的应用无法为RecSys/Search/Ads领域提供实用的技术启示。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14211v1": {
    "title": "LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14211v1",
    "arxiv_id": "2510.14211v1",
    "authors": "Beomseok Kang, Jiwon Song, Jae-Joon Kim",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 01:37:39",
    "ori_summary": "Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",
    "summary": "",
    "translation": "LiteStage：面向多阶段推理的延迟感知层跳过",
    "relevance_score": 8,
    "reasoning": "该论文提出的延迟感知层跳过技术属于Transformer架构效率优化（Enabling Transformer Tech），通过动态跳过推理阶段中的某些层来减少计算延迟。这种技术可直接应用于推荐系统和搜索中的大规模推理场景，通过智能跳过部分计算层来平衡模型性能与响应延迟，对于实时推荐和搜索服务具有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14205v1": {
    "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
    "url": "https://www.alphaxiv.org/abs/2510.14205v1",
    "arxiv_id": "2510.14205v1",
    "authors": "Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-16 01:26:38",
    "ori_summary": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences.We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews.DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios.Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
    "summary": "",
    "translation": "DPRF：一种可泛化的动态角色精炼框架，用于优化个性化大语言模型角色扮演代理与人类之间的行为对齐",
    "relevance_score": 3,
    "reasoning": "该论文主要关注LLM角色扮演代理的行为对齐，属于个性化AI交互领域。虽然涉及个性化建模，但核心应用是角色扮演而非推荐系统、搜索或广告。论文可能对个性化用户建模有间接启发，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14203v1": {
    "title": "Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14203v1",
    "arxiv_id": "2510.14203v1",
    "authors": "Ryo Masumura, Shota Orihashi, Mana Ihori, Tomohiro Tanaka, Naoki Makishima, Taiga Yamane, Naotaka Kawata, Satoshi Suzuki, Taichi Katayama",
    "categories": "cs.CV, cs.CL, cs.MM",
    "pub_date": "2025-10-16 01:21:57",
    "ori_summary": "This paper proposes a joint modeling method of the Big Five, which has long been studied, and HEXACO, which has recently attracted attention in psychology, for automatically recognizing apparent personality traits from multimodal human behavior. Most previous studies have used the Big Five for multimodal apparent personality-trait recognition. However, no study has focused on apparent HEXACO which can evaluate an Honesty-Humility trait related to displaced aggression and vengefulness, social-dominance orientation, etc. In addition, the relationships between the Big Five and HEXACO when modeled by machine learning have not been clarified. We expect awareness of multimodal human behavior to improve by considering these relationships. The key advance of our proposed method is to optimize jointly recognizing the Big Five and HEXACO. Experiments using a self-introduction video dataset demonstrate that the proposed method can effectively recognize the Big Five and HEXACO.",
    "summary": "",
    "translation": "基于多模态的显性人格特质识别：大五人格与HEXACO模型的联合建模",
    "relevance_score": 2,
    "reasoning": "该论文专注于心理学人格特质的识别，属于特定领域应用而非推荐系统、搜索或广告的核心技术。虽然人格建模在理论上可能用于个性化推荐，但论文标题未表明与Transformer架构、LLM技术或推荐系统核心算法的直接关联，且更偏向心理学应用而非技术方法创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14200v1": {
    "title": "RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following",
    "url": "https://www.alphaxiv.org/abs/2510.14200v1",
    "arxiv_id": "2510.14200v1",
    "authors": "Zhichao Wang, Andy Wong, Ruslan Belkin",
    "categories": "cs.CL",
    "pub_date": "2025-10-16 01:13:14",
    "ori_summary": "After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and RFT are applied to enhance instruction-following ability, mitigate undesired responses, improve reasoning capability and enable efficient domain adaptation with minimal data. SFT relies on the next-token prediction objective to strengthen instruction following in a base model using a large corpus of human-labeled responses. In contrast, RFT employs a RL-based approach to adapt fine-tuned reasoning models to specific domains with limited supervision. Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive SFT dataset in an RL framework, thereby improving the base model's instruction-following ability. In RLSR, the base model generates multiple responses for each prompt, and reward scores are computed as the cosine similarity in the semantic embedding space between the generated and human-labeled responses. RLSR can be utilized in multiple ways. It can directly replace SFT, achieving superior performance on instruction-following benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved a win rate of 30.73% when trained with SFT + RLSR.",
    "summary": "",
    "translation": "RLSR：在指令跟随任务中，采用监督奖励的强化学习超越监督微调",
    "relevance_score": 2,
    "reasoning": "该论文主要关注强化学习在指令跟随任务中的性能比较，属于纯粹的NLP优化技术。虽然强化学习技术本身有潜在应用价值，但论文没有明确展示与推荐系统、搜索或广告领域的直接关联，且指令跟随主要面向通用对话场景而非特定领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14184v1": {
    "title": "MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.14184v1",
    "arxiv_id": "2510.14184v1",
    "authors": "Mahmood Hegazy, Aaron Rodrigues, Azzam Naeem",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-16 00:30:08",
    "ori_summary": "We present MAFA (Multi-Agent Framework for Annotation), a production-deployed system that transforms enterprise-scale annotation workflows through configurable multi-agent collaboration. Addressing the critical challenge of annotation backlogs in financial services, where millions of customer utterances require accurate categorization, MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism. Our framework uniquely supports dynamic task adaptation, allowing organizations to define custom annotation types (FAQs, intents, entities, or domain-specific categories) through configuration rather than code changes. Deployed at JP Morgan Chase, MAFA has eliminated a 1 million utterance backlog while achieving, on average, 86% agreement with human annotators, annually saving over 5,000 hours of manual annotation work. The system processes utterances with annotation confidence classifications, which are typically 85% high, 10% medium, and 5% low across all datasets we tested. This enables human annotators to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's effectiveness across multiple datasets and languages, showing consistent improvements over traditional and single-agent annotation baselines: 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in our internal intent classification dataset and similar gains on public benchmarks. This work bridges the gap between theoretical multi-agent systems and practical enterprise deployment, providing a blueprint for organizations facing similar annotation challenges.",
    "summary": "",
    "translation": "MAFA：一种用于企业级标注的多智能体框架，具有可配置任务适应能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多智能体框架和企业级标注系统，虽然标注技术在推荐/搜索系统中可能有数据准备方面的潜在应用，但这属于间接支持技术而非核心领域进展或直接应用。论文重点在标注框架本身，没有明确涉及推荐系统、搜索广告的核心算法或LLM/Transformer技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14981v1": {
    "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
    "url": "https://www.alphaxiv.org/abs/2510.14981v1",
    "arxiv_id": "2510.14981v1",
    "authors": "Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:59",
    "ori_summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
    "summary": "",
    "translation": "用于免训练多视图图像编辑的耦合扩散采样",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于多视图图像编辑的扩散模型技术，属于纯粹的计算机视觉和图像生成领域。虽然扩散模型是重要的生成技术，但该论文没有展示与推荐系统、搜索或广告的明显联系，也不涉及异构数据建模或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14979v1": {
    "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
    "url": "https://www.alphaxiv.org/abs/2510.14979v1",
    "arxiv_id": "2510.14979v1",
    "authors": "Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:58",
    "ori_summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
    "summary": "",
    "translation": "从像素到词汇——迈向大规模原生视觉语言基元",
    "relevance_score": 8,
    "reasoning": "该论文专注于视觉语言模型(VLM)的规模化发展，这直接类比于处理推荐系统中异构数据(如上下文特征和用户序列)作为不同模态的统一建模。VLM技术在搜索和广告中具有直接应用潜力，例如多模态搜索、商品图像理解与文本描述的融合推荐，以及广告创意与用户兴趣的跨模态匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14978v1": {
    "title": "Learning an Image Editing Model without Image Editing Pairs",
    "url": "https://www.alphaxiv.org/abs/2510.14978v1",
    "arxiv_id": "2510.14978v1",
    "authors": "Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 17:59:57",
    "ori_summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
    "summary": "",
    "translation": "无需图像编辑对学习图像编辑模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像编辑技术，属于计算机视觉领域的特定应用，与推荐系统、搜索或广告的核心技术没有直接关联。即使考虑潜在的跨领域应用，该技术对于处理异构数据或提升推荐/搜索/广告系统的核心能力缺乏明确的实用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14976v1": {
    "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
    "url": "https://www.alphaxiv.org/abs/2510.14976v1",
    "arxiv_id": "2510.14976v1",
    "authors": "Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang",
    "categories": "cs.CV, cs.GR, cs.RO",
    "pub_date": "2025-10-16 17:59:56",
    "ori_summary": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
    "summary": "",
    "translation": "Ponimator：展开交互式姿态以实现多功能人-人交互动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的人体姿态动画生成，属于纯粹的视觉/图形领域。虽然涉及交互建模，但与人机交互动画直接相关，与推荐系统、搜索或广告的核心技术领域没有明显关联。该技术缺乏在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14977v1": {
    "title": "Terra: Explorable Native 3D World Model with Point Latents",
    "url": "https://www.alphaxiv.org/abs/2510.14977v1",
    "arxiv_id": "2510.14977v1",
    "authors": "Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:59:56",
    "ori_summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
    "summary": "",
    "translation": "Terra：基于点潜在表示的可探索原生3D世界模型",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于3D世界建模和点云潜在表示，属于纯粹的3D视觉领域。虽然标题提到'世界模型'，但缺乏与推荐系统、搜索或广告的直接关联。3D建模技术可能在某些特定场景（如虚拟试衣、AR购物）中有间接应用，但这种联系过于微弱且非核心。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14975v1": {
    "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14975v1",
    "arxiv_id": "2510.14975v1",
    "authors": "Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:54",
    "ori_summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
    "summary": "",
    "translation": "WithAnyone：面向可控且身份一致性的图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于可控图像生成和身份一致性，属于AIGC和内容生成领域，与广告创意生成直接相关。根据我的关注点排除标准，AIGC、内容生成以及其他纯LLM中心主题均被视为不相关，因此该论文与我的技术焦点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14974v1": {
    "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
    "url": "https://www.alphaxiv.org/abs/2510.14974v1",
    "arxiv_id": "2510.14974v1",
    "authors": "Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2025-10-16 17:59:51",
    "ori_summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
    "summary": "",
    "translation": "pi-Flow：基于策略的模仿蒸馏少步生成方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注通过模仿蒸馏实现少步生成，这属于内容生成领域，与纯粹的内容生成相关。虽然提到了策略和蒸馏技术，但没有明确展示在推荐系统、搜索或广告中的直接应用潜力，因此与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14968v1": {
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
    "url": "https://www.alphaxiv.org/abs/2510.14968v1",
    "arxiv_id": "2510.14968v1",
    "authors": "Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY",
    "pub_date": "2025-10-16 17:59:37",
    "ori_summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
    "summary": "",
    "translation": "RDD：基于检索的演示分解器，用于长周期任务中的规划器对齐",
    "relevance_score": 3,
    "reasoning": "该论文主要关注长周期任务中的规划器对齐和演示分解，虽然涉及检索技术，但其核心应用领域是机器人规划或通用任务规划，而非搜索、推荐或广告系统。检索组件可能对搜索系统有间接启发，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14965v1": {
    "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
    "url": "https://www.alphaxiv.org/abs/2510.14965v1",
    "arxiv_id": "2510.14965v1",
    "authors": "Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:59:16",
    "ori_summary": "Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
    "summary": "",
    "translation": "ChangingGrounding：变化场景中的3D视觉定位",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉定位在动态场景中的应用，这属于纯粹的3D视觉研究领域。虽然视觉定位技术在某些边缘场景中可能对搜索有间接价值，但论文明确关注3D和场景变化，与推荐系统、搜索或广告的核心技术栈没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14962v1": {
    "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
    "url": "https://www.alphaxiv.org/abs/2510.14962v1",
    "arxiv_id": "2510.14962v1",
    "authors": "Thao Nguyen, Jiaqi Ma, Fahad Shahbaz Khan, Souhaib Ben Taieb, Salman Khan",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:59:13",
    "ori_summary": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
    "summary": "",
    "translation": "RainDiff：基于逐令牌注意力扩散的端到端降水临近预报",
    "relevance_score": 2,
    "reasoning": "该论文专注于气象领域的降水预报，属于纯粹的天气预测应用，与推荐系统、搜索或广告的核心技术领域没有直接关联。虽然注意力扩散机制在技术层面有一定创新性，但该技术主要针对时空序列预测问题，在推荐、搜索或广告领域缺乏明确的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14960v1": {
    "title": "C4D: 4D Made from 3D through Dual Correspondences",
    "url": "https://www.alphaxiv.org/abs/2510.14960v1",
    "arxiv_id": "2510.14960v1",
    "authors": "Shizun Wang, Zhenxiang Jiang, Xingyi Yang, Xinchao Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:59:06",
    "ori_summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
    "summary": "",
    "translation": "C4D：通过双重对应关系从3D生成4D",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及4D数据处理和3D到4D的转换，属于计算机视觉和图形学领域，与推荐系统、搜索或广告没有直接关联。即使考虑潜在的跨模态应用，该技术主要面向视觉内容生成和处理，不属于当前关注的核心领域或使能技术范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14955v1": {
    "title": "RealDPO: Real or Not Real, that is the Preference",
    "url": "https://www.alphaxiv.org/abs/2510.14955v1",
    "arxiv_id": "2510.14955v1",
    "authors": "Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 17:58:25",
    "ori_summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
    "summary": "",
    "translation": "RealDPO：真实与否，即偏好",
    "relevance_score": 8,
    "reasoning": "该论文标题暗示了一种新的偏好优化方法（DPO的变体），这属于核心LLM技术的进展。在推荐系统、搜索和广告中，偏好学习对于个性化排名、用户行为建模和内容相关性评估至关重要，RealDPO方法可以显著提升这些领域中的用户偏好对齐和模型优化效果。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14954v1": {
    "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
    "url": "https://www.alphaxiv.org/abs/2510.14954v1",
    "arxiv_id": "2510.14954v1",
    "authors": "Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:57:53",
    "ori_summary": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
    "summary": "",
    "translation": "OmniMotion：基于连续掩码自回归的多模态运动生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于多模态运动生成，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然掩码自回归是Transformer的一种训练技术，但论文的应用场景（运动生成）在RecSys/Search/Ads领域缺乏明确的适用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14952v1": {
    "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
    "url": "https://www.alphaxiv.org/abs/2510.14952v1",
    "arxiv_id": "2510.14952v1",
    "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-16 17:57:47",
    "ori_summary": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
    "summary": "",
    "translation": "从语言到运动：基于运动潜在引导的无重定向人形机器人控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器人控制和运动生成领域，与推荐系统、搜索或广告的核心技术栈完全无关。虽然涉及语言到动作的转换，但这属于机器人学和人机交互范畴，没有明确的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14945v1": {
    "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14945v1",
    "arxiv_id": "2510.14945v1",
    "authors": "JoungBin Lee, Jaewoo Jung, Jisang Han, Takuya Narihira, Kazumi Fukuda, Junyoung Seo, Sunghwan Hong, Yuki Mitsufuji, Seungryong Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:55:25",
    "ori_summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
    "summary": "",
    "translation": "面向场景一致性相机可控视频生成的3D场景提示",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D场景理解和可控视频生成，属于纯粹的视觉生成领域。虽然涉及场景一致性建模，但缺乏与推荐系统、搜索或广告的直接关联，也不涉及Transformer架构改进或LLM技术在推荐领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14904v1": {
    "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
    "url": "https://www.alphaxiv.org/abs/2510.14904v1",
    "arxiv_id": "2510.14904v1",
    "authors": "Gabriel Fiastre, Antoine Yang, Cordelia Schmid",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2025-10-16 17:20:22",
    "ori_summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
    "summary": "",
    "translation": "MaskCaptioner：学习在视频中联合分割和描述对象轨迹",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频中的对象轨迹分割和描述，这属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然轨迹分析在理论上可以用于理解用户行为模式，但该论文没有明确展示在RecSys/Search/Ads领域的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14896v1": {
    "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14896v1",
    "arxiv_id": "2510.14896v1",
    "authors": "Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:13:33",
    "ori_summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.",
    "summary": "",
    "translation": "利用多模态大语言模型对活动描述进行可解释的半监督视频异常检测",
    "relevance_score": 2,
    "reasoning": "虽然论文涉及多模态LLM和异常检测，但核心焦点是视频分析领域，与推荐系统、搜索或广告的直接关联性较弱。视频异常检测技术可能在某些特定广告场景中有潜在应用，但缺乏明确的RecSys/Search/Ads应用路径，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14882v1": {
    "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
    "url": "https://www.alphaxiv.org/abs/2510.14882v1",
    "arxiv_id": "2510.14882v1",
    "authors": "Keli Liu, Zhendong Wang, Wengang Zhou, Shaodong Xu, Ruixiao Dong, Houqiang Li",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 17:00:59",
    "ori_summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.",
    "summary": "",
    "translation": "ScaleWeaver：通过多尺度参考注意力实现高效可控的文本到图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成领域，属于纯粹的AIGC和内容生成范畴。虽然提到了注意力机制，但这是针对图像生成任务的专用架构，没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14876v1": {
    "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
    "url": "https://www.alphaxiv.org/abs/2510.14876v1",
    "arxiv_id": "2510.14876v1",
    "authors": "Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:55:30",
    "ori_summary": "Existing collision prediction methods often fail to distinguish between ego-vehicle threats and random accidents not involving the ego vehicle, leading to excessive false alerts in real-world deployment. We present BADAS, a family of collision prediction models trained on Nexar's real-world dashcam collision dataset -- the first benchmark designed explicitly for ego-centric evaluation. We re-annotate major benchmarks to identify ego involvement, add consensus alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and comes in two variants: BADAS-Open (trained on our 1.5k public videos) and BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a forward-collision ADAS baseline while producing more realistic time-to-accident estimates. We release our BADAS-Open model weights and code, along with re-annotations of all evaluation datasets to promote ego-centric collision prediction research.",
    "summary": "",
    "translation": "BADAS：基于真实世界行车记录仪数据的上下文感知碰撞预测",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的碰撞预测，使用行车记录仪数据进行计算机视觉分析。这与搜索、推荐或广告系统的核心领域进展、LLM技术或Transformer架构改进没有直接关联，也不涉及处理异构数据模态的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14874v1": {
    "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
    "url": "https://www.alphaxiv.org/abs/2510.14874v1",
    "arxiv_id": "2510.14874v1",
    "authors": "Guangyi Han, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:52:58",
    "ori_summary": "Hand-object interaction (HOI) is fundamental for humans to express intent. Existing HOI generation research is predominantly confined to fixed grasping patterns, where control is tied to physical priors such as force closure or generic intent instructions, even when expressed through elaborate language. Such an overly general conditioning imposes a strong inductive bias for stable grasps, thus failing to capture the diversity of daily HOI. To address these limitations, we introduce Free-Form HOI Generation, which aims to generate controllable, diverse, and physically plausible HOI conditioned on fine-grained intent, extending HOI from grasping to free-form interactions, like pushing, poking, and rotating. To support this task, we construct WildO2, an in-the-wild diverse 3D HOI dataset, which includes diverse HOI derived from internet videos. Specifically, it contains 4.4k unique interactions across 92 intents and 610 object categories, each with detailed semantic annotations. Building on this dataset, we propose TOUCH, a three-stage framework centered on a multi-level diffusion model that facilitates fine-grained semantic control to generate versatile hand poses beyond grasping priors. This process leverages explicit contact modeling for conditioning and is subsequently refined with contact consistency and physical constraints to ensure realism. Comprehensive experiments demonstrate our method's ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities. The project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
    "summary": "",
    "translation": "TOUCH：文本引导的自由形式手-物体交互可控生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于手-物体交互的生成式建模，属于计算机视觉和图形学领域。虽然涉及可控生成技术，但缺乏与推荐系统、搜索或广告的明确关联，且不涉及Transformer架构改进或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14862v1": {
    "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
    "url": "https://www.alphaxiv.org/abs/2510.14862v1",
    "arxiv_id": "2510.14862v1",
    "authors": "Mihai-Cristian Pîrvu, Marius Leordeanu",
    "categories": "cs.CV, cs.DC",
    "pub_date": "2025-10-16 16:36:29",
    "ori_summary": "The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.",
    "summary": "",
    "translation": "用于机器学习的最小人工监督多模态视频数据流水线",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频数据的多模态处理流水线，这属于计算机视觉领域而非推荐系统、搜索或广告的核心技术。虽然多模态数据处理在概念上可能与VLM类比相关，但视频数据在RecSys/Search/Ads中的直接应用有限，且论文强调最小人工监督而非具体的Transformer架构或LLM应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14855v1": {
    "title": "A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation",
    "url": "https://www.alphaxiv.org/abs/2510.14855v1",
    "arxiv_id": "2510.14855v1",
    "authors": "Harsha Kotla, Arun Kumar Rajasekaran, Hannah Rana",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 16:28:21",
    "ori_summary": "Early detection of melanoma has grown to be essential because it significantly improves survival rates, but automated analysis of skin lesions still remains challenging. ABCDE, which stands for Asymmetry, Border irregularity, Color variation, Diameter, and Evolving, is a well-known classification method for skin lesions, but most deep learning mechanisms treat it as a black box, as most of the human interpretable features are not explained. In this work, we propose a deep learning framework that both classifies skin lesions into categories and also quantifies scores for each ABCD feature. It simulates the evolution of these features over time in order to represent the E aspect, opening more windows for future exploration. The A, B, C, and D values are quantified particularly within this work. Moreover, this framework also visualizes ABCD feature trajectories in latent space as skin lesions evolve from benign nevuses to malignant melanoma. The experiments are conducted using the HAM10000 dataset that contains around ten thousand images of skin lesions of varying stages. In summary, the classification worked with an accuracy of around 89 percent, with melanoma AUC being 0.96, while the feature evaluation performed well in predicting asymmetry, color variation, and diameter, though border irregularity remains more difficult to model. Overall, this work provides a deep learning framework that will allow doctors to link ML diagnoses to clinically relevant criteria, thus improving our understanding of skin cancer progression.",
    "summary": "",
    "translation": "用于皮肤病变分类、ABCDE特征量化与演化模拟的多任务深度学习框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的皮肤病变分析，属于明确的医疗应用范畴。虽然采用了深度学习技术，但其应用场景与推荐系统、搜索或广告完全无关，属于明确的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14847v1": {
    "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
    "url": "https://www.alphaxiv.org/abs/2510.14847v1",
    "arxiv_id": "2510.14847v1",
    "authors": "Meiqi Wu, Jiashu Zhu, Xiaokun Feng, Chubin Chen, Chen Zhu, Bingze Song, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:19:13",
    "ori_summary": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
    "summary": "",
    "translation": "ImagerySearch：超越语义依赖约束的自适应测试时搜索视频生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成中的搜索技术，虽然涉及搜索概念，但核心是内容生成（AIGC）领域，属于明确的无关主题。论文标题强调超越语义约束的视频生成，这与推荐系统、搜索或广告中的排名和检索任务没有直接关联，且视频生成属于被排除的AIGC范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14845v1": {
    "title": "Backdoor Unlearning by Linear Task Decomposition",
    "url": "https://www.alphaxiv.org/abs/2510.14845v1",
    "arxiv_id": "2510.14845v1",
    "authors": "Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2025-10-16 16:18:07",
    "ori_summary": "Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.",
    "summary": "",
    "translation": "通过线性任务分解实现后门遗忘",
    "relevance_score": 1,
    "reasoning": "该论文关注后门攻击和模型遗忘，属于安全领域，与我的关注点（推荐系统、搜索、广告中的核心进展、LLM技术及其应用）无关。后门遗忘主要涉及模型安全和隐私保护，这些被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14836v1": {
    "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
    "url": "https://www.alphaxiv.org/abs/2510.14836v1",
    "arxiv_id": "2510.14836v1",
    "authors": "Yixuan Li, Yuhui Chen, Mingcai Zhou, Haoran Li",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-16 16:11:18",
    "ori_summary": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.",
    "summary": "",
    "translation": "QDepth-VLA：量化深度预测作为视觉-语言-动作模型的辅助监督",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉-语言-动作模型中的深度预测任务，属于机器人控制和具身智能领域。虽然涉及多模态学习概念，但与推荐系统、搜索或广告的核心技术需求缺乏直接关联。量化深度预测作为辅助监督的方法难以转化为RecSys/Search/Ads领域的实际应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14831v1": {
    "title": "Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data",
    "url": "https://www.alphaxiv.org/abs/2510.14831v1",
    "arxiv_id": "2510.14831v1",
    "authors": "Qi Chen, Xinze Zhou, Chen Liu, Hao Chen, Wenxuan Li, Zekun Jiang, Ziyan Huang, Yuxuan Zhao, Dexin Yu, Junjun He, Yefeng Zheng, Ling Shao, Alan Yuille, Zongwei Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:08:09",
    "ori_summary": "AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation--based on lessons from the JHH dataset--for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests.",
    "summary": "",
    "translation": "规模化肿瘤分割：来自真实与合成数据的最佳经验",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像中的肿瘤分割技术，属于医疗领域的计算机视觉应用。这与我的关注领域（推荐系统、搜索、广告）完全无关，且明确排除了医疗、生物等特定领域应用。论文内容不涉及任何与推荐、搜索或广告相关的技术或潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14823v1": {
    "title": "FraQAT: Quantization Aware Training with Fractional bits",
    "url": "https://www.alphaxiv.org/abs/2510.14823v1",
    "arxiv_id": "2510.14823v1",
    "authors": "Luca Morreale, Alberto Gil C. P. Ramos, Malcolm Chadwick, Mehid Noroozi, Ruchika Chavhan, Abhinav Mehrotra, Sourav Bhattacharya",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 16:01:08",
    "ori_summary": "State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \\eg, in \\INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\\short) approach. The novelty is a simple yet effective idea: we progressively reduce the model's precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \\short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
    "summary": "",
    "translation": "FraQAT：使用分数位数的量化感知训练",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种新的量化方法，允许使用分数位数进行模型压缩，这属于Transformer架构效率提升的核心技术。在推荐系统和搜索广告领域，这种高效的量化技术可以显著降低大规模模型的部署成本，提高推理速度，同时保持模型性能，对实际工业应用具有重要价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14819v1": {
    "title": "Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14819v1",
    "arxiv_id": "2510.14819v1",
    "authors": "Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 15:55:28",
    "ori_summary": "Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \\textbf{P}erception and explicit \\textbf{R}oute choice modeling for effective \\textbf{Traj}ectory representation learning, dubbed \\textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: https://anonymous.4open.science/r/PRTraj.",
    "summary": "",
    "translation": "环境感知与路径选择建模相统一的轨迹表示学习",
    "relevance_score": 3,
    "reasoning": "该论文专注于轨迹表示学习，虽然轨迹建模在移动推荐和位置服务中有潜在应用，但论文标题明确强调环境感知和路径选择建模，这更偏向于交通规划、自动驾驶等特定领域。对于推荐系统、搜索或广告的核心关注点来说，这种轨迹建模的直接相关性有限，且没有明确涉及LLM或Transformer技术在这些领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14803v1": {
    "title": "Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks",
    "url": "https://www.alphaxiv.org/abs/2510.14803v1",
    "arxiv_id": "2510.14803v1",
    "authors": "Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon Płotka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub Prządo, Ibrahim E. Hamacı, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, Jarosław B. Ćwikła, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 15:35:44",
    "ori_summary": "Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types. We plan to release our trained models, code, and dataset at https://github.com/MrGiovanni/R-Super",
    "summary": "",
    "translation": "利用更多报告、更少掩码扩展人工智能用于多肿瘤早期检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的多肿瘤早期检测应用，属于明确的医疗领域特定应用。虽然涉及人工智能扩展技术，但核心应用场景（肿瘤检测）与推荐系统、搜索或广告领域完全无关，且医学应用属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14800v1": {
    "title": "Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images",
    "url": "https://www.alphaxiv.org/abs/2510.14800v1",
    "arxiv_id": "2510.14800v1",
    "authors": "Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 15:32:05",
    "ori_summary": "Colorectal cancer (CRC) remains the third most prevalent malignancy globally, with approximately 154,000 new cases and 54,000 projected deaths anticipated for 2025. The recent advancement of foundation models in computational pathology has been largely propelled by task agnostic methodologies that can overlook organ-specific crucial morphological patterns that represent distinct biological processes that can fundamentally influence tumor behavior, therapeutic response, and patient outcomes. The aim of this study is to develop a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated Spatial Morphology), that incorporates a continuous variability spectrum within each distinct morphology to characterize phenotypic diversity and reflecting the principle that malignant transformation occurs through incremental evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained on 8.74 million histological images extracted from surgical resection specimens of 424 patients with stage III CRC. PRISM achieved superior prognostic performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%; HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific methods by 15% and AI foundation models by ~23% accuracy. It showed sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable performance across clinicopathological subgroups, with minimal accuracy fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens, replicating the Alliance cohort finding of no survival difference between treatments.",
    "summary": "",
    "translation": "基于H&E全切片图像的结直肠癌五年生存预测形态学感知预后模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的结直肠癌生存预测，属于明确的医学/生物学应用范畴，与推荐系统、搜索或广告领域完全无关。论文涉及H&E全切片图像分析和癌症预后，这些主题在Irrelevant Topics中明确排除，没有任何技术要素与我的当前关注点相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14792v1": {
    "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14792v1",
    "arxiv_id": "2510.14792v1",
    "authors": "Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 15:27:10",
    "ori_summary": "Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art.",
    "summary": "",
    "translation": "CoT-PL：视觉思维链推理与伪标签方法相结合用于开放词汇目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的开放词汇目标检测，虽然涉及视觉推理和伪标签技术，但与推荐系统、搜索或广告的核心领域进展缺乏直接关联。视觉思维链推理可能对多模态推荐有一定启发，但论文的计算机视觉焦点使其在所列关注领域中的实际应用潜力有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14770v1": {
    "title": "MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks",
    "url": "https://www.alphaxiv.org/abs/2510.14770v1",
    "arxiv_id": "2510.14770v1",
    "authors": "Zhang Nengbo, Hann Woei Ho, Ye Zhou",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 15:06:51",
    "ori_summary": "Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in environments, where conventional radio-based methods suffer from spectrum congestion, jamming, and high power consumption. Inspired by the waggle dance of honeybees, which efficiently communicate the location of food sources without sound or contact, we propose a novel visual communication framework for MAV swarms using motion-based signaling. In this framework, MAVs convey information, such as heading and distance, through deliberate flight patterns, which are passively captured by event cameras and interpreted using a predefined visual codebook of four motion primitives: vertical (up/down), horizontal (left/right), left-to-up-to-right, and left-to-down-to-right, representing control symbols (``start'', ``end'', ``1'', ``0''). To decode these signals, we design an event frame-based segmentation model and a lightweight Spiking Neural Network (SNN) for action recognition. An integrated decoding algorithm then combines segmentation and classification to robustly interpret MAV motion sequences. Experimental results validate the framework's effectiveness, which demonstrates accurate decoding and low power consumption, and highlights its potential as an energy-efficient alternative for MAV communication in constrained environments.",
    "summary": "",
    "translation": "MoCom：基于运动的多MAV间视觉通信，采用事件视觉与脉冲神经网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于微型飞行器（MAV）间的运动视觉通信技术，涉及事件视觉和脉冲神经网络，这些属于专门的机器人视觉和神经形态计算领域。与搜索、推荐、广告系统的核心进展或使能技术没有明显关联，也不涉及Transformer架构、LLM应用或异构数据统一建模等关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14765v1": {
    "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
    "url": "https://www.alphaxiv.org/abs/2510.14765v1",
    "arxiv_id": "2510.14765v1",
    "authors": "Giuseppe Lorenzo Catalano, Agata Marta Soccini",
    "categories": "cs.CV, cs.AI, cs.GR",
    "pub_date": "2025-10-16 15:02:05",
    "ori_summary": "Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.",
    "summary": "",
    "translation": "修复红色星球：用于虚拟现实中火星环境重建的扩散模型",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于火星环境重建和虚拟现实应用，属于纯粹的视觉和3D视觉领域。虽然使用了扩散模型技术，但该应用与推荐系统、搜索或广告没有任何直接或潜在的关联，完全超出了当前关注的技术领域范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14753v1": {
    "title": "LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement",
    "url": "https://www.alphaxiv.org/abs/2510.14753v1",
    "arxiv_id": "2510.14753v1",
    "authors": "Xu Wu, Zhihui Lai, Xianxu Hou, Jie Zhou, Ya-nan Zhang, Linlin Shen",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:54:42",
    "ori_summary": "Low-light image enhancement (LLIE) aims to improve illumination while preserving high-quality color and texture. However, existing methods often fail to extract reliable feature representations due to severely degraded pixel-level information under low-light conditions, resulting in poor texture restoration, color inconsistency, and artifact. To address these challenges, we propose LightQANet, a novel framework that introduces quantized and adaptive feature learning for low-light enhancement, aiming to achieve consistent and robust image quality across diverse lighting conditions. From the static modeling perspective, we design a Light Quantization Module (LQM) to explicitly extract and quantify illumination-related factors from image features. By enforcing structured light factor learning, LQM enhances the extraction of light-invariant representations and mitigates feature inconsistency across varying illumination levels. From the dynamic adaptation perspective, we introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors into learnable prompts to dynamically guide the feature learning process. LAPM enables the model to flexibly adapt to complex and continuously changing lighting conditions, further improving image enhancement. Extensive experiments on multiple low-light datasets demonstrate that our method achieves state-of-the-art performance, delivering superior qualitative and quantitative results across various challenging lighting scenarios.",
    "summary": "",
    "translation": "LightQANet：面向低光照图像增强的量化与自适应特征学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的低光照图像增强技术，涉及量化和自适应特征学习方法。虽然标题提到'特征学习'，但这是纯粹的视觉处理任务，与推荐系统、搜索或广告中的排序、用户建模、内容理解等核心问题没有直接关联，也不涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14741v1": {
    "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
    "url": "https://www.alphaxiv.org/abs/2510.14741v1",
    "arxiv_id": "2510.14741v1",
    "authors": "Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato",
    "categories": "cs.CV, cs.AI, I.2.m",
    "pub_date": "2025-10-16 14:43:25",
    "ori_summary": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
    "summary": "",
    "translation": "DEXTER：基于扩散引导解释与文本推理的视觉模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉模型的解释性方法，结合扩散模型和文本推理。虽然标题提到视觉模型，但缺乏与推荐系统、搜索或广告的直接关联。扩散引导的解释方法可能在视觉内容理解方面有应用，但未明确展示在RecSys/Search/Ads领域的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14737v1": {
    "title": "Free-Grained Hierarchical Recognition",
    "url": "https://www.alphaxiv.org/abs/2510.14737v1",
    "arxiv_id": "2510.14737v1",
    "authors": "Seulki Park, Zilin Wang, Stella X. Yu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:35:18",
    "ori_summary": "Hierarchical image classification predicts labels across a semantic taxonomy, but existing methods typically assume complete, fine-grained annotations, an assumption rarely met in practice. Real-world supervision varies in granularity, influenced by image quality, annotator expertise, and task demands; a distant bird may be labeled Bird, while a close-up reveals Bald eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet and structured into cognitively inspired basic, subordinate, and fine-grained levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic, mixed-granularity labels reflecting human annotation behavior. We propose free-grain learning, with heterogeneous supervision across instances. We develop methods that enhance semantic guidance via pseudo-attributes from vision-language models and visual guidance via semi-supervised learning. These, along with strong baselines, substantially improve performance under mixed supervision. Together, our benchmark and methods advance hierarchical classification under real-world constraints.",
    "summary": "",
    "translation": "细粒度层次化识别",
    "relevance_score": 2,
    "reasoning": "该标题涉及细粒度和层次化识别概念，可能属于计算机视觉或模式识别领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然层次化结构在推荐系统中用于组织商品分类，但该论文没有明确表明与Transformer架构、LLM技术或推荐系统应用的连接。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14726v1": {
    "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14726v1",
    "arxiv_id": "2510.14726v1",
    "authors": "Dingzhou Xie, Rushi Lan, Cheng Pang, Enhao Ning, Jiahao Zeng, Wei Zheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:25:21",
    "ori_summary": "Recent object detection methods have made remarkable progress by leveraging attention mechanisms to improve feature discriminability. However, most existing approaches are confined to refining single-layer or fusing dual-layer features, overlooking the rich inter-layer dependencies across multi-scale representations. This limits their ability to capture comprehensive contextual information essential for detecting objects with large scale variations. In this paper, we propose a novel Cross-Layer Feature Self-Attention Module (CFSAM), which holistically models both local and global dependencies within multi-scale feature maps. CFSAM consists of three key components: a convolutional local feature extractor, a Transformer-based global modeling unit that efficiently captures cross-layer interactions, and a feature fusion mechanism to restore and enhance the original representations. When integrated into the SSD300 framework, CFSAM significantly boosts detection performance, achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the module accelerates convergence during training without introducing substantial computational overhead. Our work highlights the importance of explicit cross-layer attention modeling in advancing multi-scale object detection.",
    "summary": "",
    "translation": "用于多尺度目标检测的跨层特征自注意力模块",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的目标检测任务，虽然涉及注意力机制，但其应用场景和核心问题与推荐系统、搜索或广告领域相距甚远。论文提出的跨层特征自注意力模块可能对视觉特征建模有一定启发，但这种启发过于间接，难以直接应用于推荐、搜索或广告中的用户行为序列建模或异构特征融合。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14713v1": {
    "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
    "url": "https://www.alphaxiv.org/abs/2510.14713v1",
    "arxiv_id": "2510.14713v1",
    "authors": "Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig",
    "categories": "cs.CV, cs.AI, eess.IV",
    "pub_date": "2025-10-16 14:11:52",
    "ori_summary": "Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.",
    "summary": "",
    "translation": "历史影像中相机运动分类：深度视频模型的比较研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于历史影像中的相机运动分类，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。论文内容涉及视频模型比较，但没有展示在异构数据处理、Transformer架构改进或LLM应用方面的潜力，无法找到在RecSys/Search/Ads领域的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14709v1": {
    "title": "Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery",
    "url": "https://www.alphaxiv.org/abs/2510.14709v1",
    "arxiv_id": "2510.14709v1",
    "authors": "Caleb Robinson, Kimberly T. Goetz, Christin B. Khan, Meredith Sackett, Kathleen Leonard, Rahul Dodhia, Juan M. Lavista Ferres",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 14:10:51",
    "ori_summary": "Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. \"interesting points\". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at https://github.com/microsoft/whales.",
    "summary": "",
    "translation": "鲸在何处：一种人机协同检测方法用于高分辨率卫星图像中的鲸鱼识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的目标检测任务，应用于卫星图像中的鲸鱼识别，与推荐系统、搜索或广告领域完全无关。论文内容涉及特定领域的视觉检测方法，没有任何技术组件可以转化或应用于我的核心关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14705v1": {
    "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
    "url": "https://www.alphaxiv.org/abs/2510.14705v1",
    "arxiv_id": "2510.14705v1",
    "authors": "Seungjoo Shin, Jaesik Park, Sunghyun Cho",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 14:10:02",
    "ori_summary": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
    "summary": "",
    "translation": "利用学习到的图像先验进行3D高斯压缩",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D高斯压缩和图像先验学习，属于计算机视觉和3D图形领域。虽然提到了学习到的先验，但这与推荐系统、搜索或广告中的异构数据建模没有明确关联。该技术主要面向3D视觉应用，在我的关注领域中缺乏直接相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14672v1": {
    "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.14672v1",
    "arxiv_id": "2510.14672v1",
    "authors": "Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:29:02",
    "ori_summary": "In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io",
    "summary": "",
    "translation": "VTimeCoT：通过绘图思考实现视频时序定位与推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频时序定位和推理任务，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然思维链（CoT）方法在LLMs中有应用，但该工作专注于视频模态，缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14668v1": {
    "title": "WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging",
    "url": "https://www.alphaxiv.org/abs/2510.14668v1",
    "arxiv_id": "2510.14668v1",
    "authors": "Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Sami Azam, Asif Karim, Jemima Beissbarth, Amanda Leach",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:22:51",
    "ori_summary": "Knowledge distillation (KD) has traditionally relied on a static teacher-student framework, where a large, well-trained teacher transfers knowledge to a single student model. However, these approaches often suffer from knowledge degradation, inefficient supervision, and reliance on either a very strong teacher model or large labeled datasets, which limits their effectiveness in real-world, limited-data scenarios. To address these, we present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that redefines knowledge transfer through a structured sequence of interconnected models. Unlike conventional KD, it forms a progressive distillation chain, where each model not only learns from its predecessor but also refines the knowledge before passing it forward. This structured knowledge transfer further enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD. Each model in the distillation chain is trained on only a fraction of the dataset and demonstrates that effective learning can be achieved with minimal supervision. Extensive evaluations across four otoscopic imaging datasets demonstrate that it not only matches but in many cases surpasses the performance of existing supervised methods. Experimental results on two other datasets further underscore its generalization across diverse medical imaging modalities, including microscopic and magnetic resonance imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of up to +23% over a single backbone trained on the same limited data, which highlights its potential for real-world adoption.",
    "summary": "",
    "translation": "WeCKD：用于高效多模态医学成像的弱监督链式蒸馏网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于多模态医学成像，属于明确的医学领域应用，与搜索、推荐或广告系统完全无关。虽然提到了多模态和蒸馏技术，但这些技术被应用于医学成像这一无关领域，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14661v1": {
    "title": "EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)",
    "url": "https://www.alphaxiv.org/abs/2510.14661v1",
    "arxiv_id": "2510.14661v1",
    "authors": "Weikang Yu, Vincent Nwazelibe, Xianping Ma, Xiaokang Zhang, Richard Gloaguen, Xiao Xiang Zhu, Pedram Ghamisi",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 13:15:53",
    "ori_summary": "Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at https://github.com/EricYu97/EuroMineNet.",
    "summary": "",
    "translation": "EuroMineNet：一个用于欧盟时空采矿足迹分析的多时序哨兵2号基准数据集（2015-2024）",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像分析和地理空间数据挖掘，属于纯粹的计算机视觉应用领域。虽然涉及时序数据分析，但其核心关注点（采矿足迹监测）与推荐系统、搜索或广告领域没有任何直接或间接的技术关联，也不涉及LLM、Transformer架构或异构数据建模技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14657v1": {
    "title": "Decorrelation Speeds Up Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.14657v1",
    "arxiv_id": "2510.14657v1",
    "authors": "Kieran Carrigg, Rob van Gastel, Melda Yeghaian, Sander Dalm, Faysal Boughorbel, Marcel van Gerven",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 13:13:12",
    "ori_summary": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.",
    "summary": "",
    "translation": "去相关加速视觉Transformer",
    "relevance_score": 6,
    "reasoning": "该论文属于'赋能Transformer技术'范畴，研究Transformer架构的效率优化。去相关技术通过减少特征冗余来加速视觉Transformer，这种效率提升方法可以直接应用于推荐系统和搜索中的大规模Transformer模型，减少计算开销并提高推理速度。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14648v1": {
    "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
    "url": "https://www.alphaxiv.org/abs/2510.14648v1",
    "arxiv_id": "2510.14648v1",
    "authors": "Xinyao Liao, Xianfang Zeng, Ziye Song, Zhoujie Fu, Gang Yu, Guosheng Lin",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 13:02:11",
    "ori_summary": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
    "summary": "",
    "translation": "基于指令的视频编辑中未配对片段的情境学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频编辑领域的指令跟随和情境学习，属于纯粹的视觉内容生成应用。虽然涉及指令理解，但核心是视频内容创作而非推荐、搜索或广告中的排名、检索或个性化任务。没有明显的技术组件（如Transformer架构改进）可以应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14634v1": {
    "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.14634v1",
    "arxiv_id": "2510.14634v1",
    "authors": "Jihyun Yu, Yoojin Oh, Wonho Bae, Mingyu Kim, Junhyug Noh",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:46:53",
    "ori_summary": "Test-time adaptation (TTA) aims to correct performance degradation of deep models under distribution shifts by updating models or inputs using unlabeled test data. Input-only diffusion-based TTA methods improve robustness for classification to corruptions but rely on gradient guidance, limiting exploration and generalization across distortion types. We propose SteeringTTA, an inference-only framework that adapts Feynman-Kac steering to guide diffusion-based input adaptation for classification with rewards driven by pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by a combination of cumulative top-K probabilities and an entropy schedule, to balance exploration and confidence. On ImageNet-C, SteeringTTA consistently outperforms the baseline without any model updates or source data.",
    "summary": "",
    "translation": "SteeringTTA：引导扩散轨迹以实现鲁棒的测试时适应",
    "relevance_score": 2,
    "reasoning": "该论文主要关注扩散模型的测试时适应技术，属于生成模型领域。虽然扩散模型在内容生成方面有应用，但该论文没有明确展示与推荐系统、搜索或广告的直接关联。测试时适应技术可能对模型鲁棒性有潜在价值，但缺乏具体的应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14630v1": {
    "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14630v1",
    "arxiv_id": "2510.14630v1",
    "authors": "Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, Björn Ommer",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:43:03",
    "ori_summary": "We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained jointly using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring the latent space remains smooth and suitable for generation. Our single-token formulation resolves spatial redundancies of 2D latent spaces and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and naturally extends to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling.",
    "summary": "",
    "translation": "将自监督表示作为潜在空间进行高效生成",
    "relevance_score": 6,
    "reasoning": "该论文涉及自监督表示学习（属于Enabling LLM Tech范畴）和高效生成技术。在推荐系统或搜索中，自监督表示可以作为用户行为序列或商品特征的潜在编码，而高效生成技术可以用于快速生成个性化推荐内容或搜索结果的多样化展示。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14627v1": {
    "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement",
    "url": "https://www.alphaxiv.org/abs/2510.14627v1",
    "arxiv_id": "2510.14627v1",
    "authors": "Yao Zhong, Hanzhi Chen, Simon Schaefer, Anran Zhang, Stefan Leutenegger",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2025-10-16 12:38:14",
    "ori_summary": "Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.",
    "summary": "",
    "translation": "GOPLA：通过人类排列的合成增强实现可泛化的物体放置学习",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的物体放置任务，虽然涉及合成数据增强，但其核心是视觉场景理解和物体布局生成，与推荐系统、搜索或广告的核心技术没有直接关联。即使考虑VLM类比，该工作更偏向纯粹的视觉空间布局问题，而非处理推荐/搜索领域中的异构数据模态统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14624v1": {
    "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
    "url": "https://www.alphaxiv.org/abs/2510.14624v1",
    "arxiv_id": "2510.14624v1",
    "authors": "Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:34:38",
    "ori_summary": "Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.",
    "summary": "",
    "translation": "高效视频采样：通过剪枝时间冗余令牌加速视觉语言模型推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型(VLM)的视频处理效率优化，属于纯粹的视觉领域效率改进。虽然提到了推理加速技术，但缺乏明确的搜索、推荐或广告应用场景。视频采样和冗余令牌剪枝技术主要针对视觉模态优化，与处理异构数据的VLM类比理念关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14617v1": {
    "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.14617v1",
    "arxiv_id": "2510.14617v1",
    "authors": "Ning Ding, Keisuke Fujii, Toru Tamaki",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 12:24:51",
    "ori_summary": "Tactical understanding in badminton involves interpreting not only individual actions but also how tactics are dynamically executed over time. In this paper, we propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and temporal multi-scale video captioning in badminton, capable of generating shot-level captions that describe individual actions and tactic-level captions that capture how these actions unfold over time within a tactical execution. We also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning dataset containing 5,494 shot captions and 544 tactic captions. Shot2Tactic-Caption adopts a dual-branch design, with both branches including a visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based decoder to generate shot and tactic captions. To support tactic captioning, we additionally introduce a Tactic Unit Detector that identifies valid tactic units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic captioning, we further incorporate a shot-wise prompt-guided mechanism, where the predicted tactic type and state are embedded as prompts and injected into the decoder via cross-attention. The shot-wise prompt-guided mechanism enables our system not only to describe successfully executed tactics but also to capture tactical executions that are temporarily interrupted and later resumed. Experimental results demonstrate the effectiveness of our framework in generating both shot and tactic captions. Ablation studies show that the ResNet50-based spatio-temporal encoder outperforms other variants, and that shot-wise prompt structuring leads to more coherent and accurate tactic captioning.",
    "summary": "",
    "translation": "Shot2Tactic-Caption：用于战术理解的多尺度羽毛球视频描述生成",
    "relevance_score": 2,
    "reasoning": "该论文专注于体育视频分析和战术理解，属于纯粹的计算机视觉应用领域。虽然涉及多模态建模，但其核心是体育视频的战术分析，与推荐系统、搜索或广告领域没有明确的关联或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14605v1": {
    "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
    "url": "https://www.alphaxiv.org/abs/2510.14605v1",
    "arxiv_id": "2510.14605v1",
    "authors": "Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 12:10:00",
    "ori_summary": "Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
    "summary": "",
    "translation": "基于知识的视觉问答：融合多模态处理、检索与过滤",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉问答任务，属于计算机视觉与自然语言处理的交叉领域，与推荐系统、搜索或广告的核心技术关联度较低。虽然涉及多模态处理和检索技术，但其应用场景和问题定义与RecSys/Search/Ads领域差异较大，潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14596v1": {
    "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering",
    "url": "https://www.alphaxiv.org/abs/2510.14596v1",
    "arxiv_id": "2510.14596v1",
    "authors": "Hugo Markoff, Jevgenijs Galaktionovs",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:59:18",
    "ori_summary": "Camera traps generate millions of wildlife images, yet many datasets contain species that are absent from existing classifiers. This work evaluates zero-shot approaches for organizing unlabeled wildlife imagery using self-supervised vision transformers, developed and tested within the Animal Detect platform for camera trap analysis. We compare unsupervised clustering methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor) combined with dimensionality reduction techniques (PCA, UMAP), and we demonstrate continuous 1D similarity ordering via t-SNE projection. On a 5-species test set with ground truth labels used only for evaluation, DINOv2 with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent for fish across 1,500 images. Based on these findings, we deployed continuous similarity ordering in production, enabling rapid exploratory analysis and accelerating manual annotation workflows for biodiversity monitoring.",
    "summary": "",
    "translation": "使用视觉变换器的零样本野生动物分类：评估聚类与连续相似性排序",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的野生动物分类任务，使用视觉变换器进行零样本学习和相似性排序。虽然涉及变换器架构，但其应用领域（野生动物分类）与推荐系统、搜索或广告没有直接关联，且没有明确的技术迁移路径或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14594v1": {
    "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2510.14594v1",
    "arxiv_id": "2510.14594v1",
    "authors": "Hugo Markoff, Jevgenijs Galaktionovs",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:57:07",
    "ori_summary": "State-of-the-art animal classification models like SpeciesNet provide predictions across thousands of species but use conservative rollup strategies, resulting in many animals labeled at high taxonomic levels rather than species. We present a hierarchical re-classification system for the Animal Detect platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP embeddings and metric learning to refine high-level taxonomic labels toward species-level identification. Our five-stage pipeline (high-confidence acceptance, bird override, centroid building, triplet-loss metric learning, and adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC Desert Lion Conservation dataset (4,018 images, 15,031 detections). After recovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify 456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving species-level identification for 64.9 percent",
    "summary": "",
    "translation": "层次化重分类：将动物分类模型与视觉变换器相结合",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的动物分类任务，使用视觉变换器进行图像识别。虽然涉及Transformer架构，但纯视觉应用与推荐系统、搜索或广告没有直接关联，且不涉及多模态建模或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14588v1": {
    "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
    "url": "https://www.alphaxiv.org/abs/2510.14588v1",
    "arxiv_id": "2510.14588v1",
    "authors": "Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 11:50:38",
    "ori_summary": "Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.",
    "summary": "",
    "translation": "STANCE：通过稀疏到密集锚定编码实现运动一致性的视频生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于视频生成技术，属于纯粹的视觉生成领域，与推荐系统、搜索或广告的核心技术无关。虽然提到了编码技术，但这是针对视频数据的特定处理，没有明确的潜在应用场景可以连接到RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14576v1": {
    "title": "CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification",
    "url": "https://www.alphaxiv.org/abs/2510.14576v1",
    "arxiv_id": "2510.14576v1",
    "authors": "Dongwook Lee, Sol Han, Jinwhan Kim",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:36:54",
    "ori_summary": "This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based multi-branch neural network for vehicle re-identification. The proposed model addresses the challenge of learning discriminative and complementary features from three-dimensional point clouds to distinguish between vehicles. CALM-Net employs a multi-branch architecture that integrates edge convolution, point attention, and a curvature embedding that characterizes local surface variation in point clouds. By combining these mechanisms, the model learns richer geometric and contextual features that are well suited for the re-identification task. Experimental evaluation on the large-scale nuScenes dataset demonstrates that CALM-Net achieves a mean re-identification accuracy improvement of approximately 1.97\\% points compared with the strongest baseline in our study. The results confirms the effectiveness of incorporating curvature information into deep learning architectures and highlight the benefit of multi-branch feature learning for LiDAR point cloud-based vehicle re-identification.",
    "summary": "",
    "translation": "CALM-Net：基于曲率感知LiDAR点云的多分支神经网络用于车辆重识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的车辆重识别任务，使用LiDAR点云数据和曲率感知技术。虽然车辆重识别在广义上可能与搜索系统相关，但该工作主要涉及3D视觉和点云处理，没有明确展示与推荐系统、搜索或广告的关联，也不涉及LLM、Transformer架构或异构数据建模等核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14564v1": {
    "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
    "url": "https://www.alphaxiv.org/abs/2510.14564v1",
    "arxiv_id": "2510.14564v1",
    "authors": "Junyi Wu, Jiaming Xu, Jinhao Li, Yongkang Zhou, Jiayi Pan, Xingyang Li, Guohao Dai",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:16:58",
    "ori_summary": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting. To tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory. Extensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
    "summary": "",
    "translation": "BalanceGS：面向GPU高效3D高斯泼溅训练的算法-系统协同设计",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于3D高斯泼溅训练的GPU效率优化，属于计算机图形学和3D视觉领域。虽然涉及算法-系统协同设计，但与推荐系统、搜索或广告的核心技术没有明显关联。该技术主要面向3D内容生成和渲染，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14560v1": {
    "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
    "url": "https://www.alphaxiv.org/abs/2510.14560v1",
    "arxiv_id": "2510.14560v1",
    "authors": "Yulin Zhang, Cheng Shi, Yang Wang, Sibei Yang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 11:11:13",
    "ori_summary": "Envision an AI capable of functioning in human-like settings, moving beyond mere observation to actively understand, anticipate, and proactively respond to unfolding events. Towards this vision, we focus on the innovative task where, given ego-streaming video input, an assistant proactively answers diverse, evolving questions at the opportune moment, while maintaining synchronized perception and reasoning. This task embodies three key properties: (1) Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized Efficiency. To evaluate and address these properties, we first introduce ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a novel framework designed for their rigorous assessment. Secondly, we propose a comprehensive technical pipeline to enable models to tackle this challenging task. This pipeline comprises: (1) a data engine, (2) a multi-stage training strategy, and (3) a proactive dynamic compression technique. Our proposed model effectively addresses these critical properties while outperforming multiple baselines across diverse online and offline benchmarks. Project Page:https://zhangyl4.github.io/publications/eyes-wide-open/",
    "summary": "",
    "translation": "睁大眼睛：面向流媒体视频的自我主动视频大语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频流媒体场景下的主动式视频理解，属于计算机视觉与视频处理的交叉领域。虽然涉及LLM技术，但其核心应用场景（流媒体视频分析）与推荐系统、搜索或广告的关联性较弱，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14553v1": {
    "title": "Consistent text-to-image generation via scene de-contextualization",
    "url": "https://www.alphaxiv.org/abs/2510.14553v1",
    "arxiv_id": "2510.14553v1",
    "authors": "Song Tang, Peihao Gong, Kunyu Li, Kai Guo, Boyu Wang, Mao Ye, Jianwei Zhang, Xiatian Zhu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:54:49",
    "ori_summary": "Consistent text-to-image (T2I) generation seeks to produce identity-preserving images of the same subject across diverse scenes, yet it often fails due to a phenomenon called identity (ID) shift. Previous methods have tackled this issue, but typically rely on the unrealistic assumption of knowing all target scenes in advance. This paper reveals that a key source of ID shift is the native correlation between subject and scene context, called scene contextualization, which arises naturally as T2I models fit the training distribution of vast natural images. We formally prove the near-universality of this scene-ID correlation and derive theoretical bounds on its strength. On this basis, we propose a novel, efficient, training-free prompt embedding editing approach, called Scene De-Contextualization (SDeC), that imposes an inversion process of T2I's built-in scene contextualization. Specifically, it identifies and suppresses the latent scene-ID correlation within the ID prompt's embedding by quantifying the SVD directional stability to adaptively re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene use (one scene per prompt) without requiring prior access to all target scenes. This makes it a highly flexible and general solution well-suited to real-world applications where such prior knowledge is often unavailable or varies over time. Experiments demonstrate that SDeC significantly enhances identity preservation while maintaining scene diversity.",
    "summary": "",
    "translation": "通过场景去上下文化实现一致的文本到图像生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于文本到图像生成技术，属于纯粹的AIGC和内容生成领域。虽然涉及生成模型，但该方法专门针对视觉内容生成的一致性改进，没有明确的推荐系统、搜索或广告应用潜力。该技术主要服务于创意内容生成，而非排名、检索或个性化推荐等核心业务场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14543v1": {
    "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14543v1",
    "arxiv_id": "2510.14543v1",
    "authors": "Ziqi Jiang, Yanghao Wang, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:32:48",
    "ori_summary": "Aligning features from different modalities, is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment. It is insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have demonstrated that FMA can consistently yield significant performance gains across various benchmarks and backbones, particularly on challenging datasets.",
    "summary": "",
    "translation": "探索跨模态流在少样本学习中的应用",
    "relevance_score": 7,
    "reasoning": "该论文研究跨模态学习，与'VLM类比处理异构数据'焦点直接相关，可将用户特征和序列视为不同模态进行统一建模。跨模态技术在推荐系统中具有应用潜力，例如处理用户行为序列与上下文特征的融合，或处理多模态内容推荐场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14536v1": {
    "title": "Exploring Image Representation with Decoupled Classical Visual Descriptors",
    "url": "https://www.alphaxiv.org/abs/2510.14536v1",
    "arxiv_id": "2510.14536v1",
    "authors": "Chenyuan Qu, Hao Chen, Jianbo Jiao",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:27:55",
    "ori_summary": "Exploring and understanding efficient image representations is a long-standing challenge in computer vision. While deep learning has achieved remarkable progress across image understanding tasks, its internal representations are often opaque, making it difficult to interpret how visual information is processed. In contrast, classical visual descriptors (e.g. edge, colour, and intensity distribution) have long been fundamental to image analysis and remain intuitively understandable to humans. Motivated by this gap, we ask a central question: Can modern learning benefit from these classical cues? In this paper, we answer it with VisualSplit, a framework that explicitly decomposes images into decoupled classical descriptors, treating each as an independent but complementary component of visual knowledge. Through a reconstruction-driven pre-training scheme, VisualSplit learns to capture the essence of each visual descriptor while preserving their interpretability. By explicitly decomposing visual attributes, our method inherently facilitates effective attribute control in various advanced visual tasks, including image generation and editing, extending beyond conventional classification and segmentation, suggesting the effectiveness of this new learning approach for visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.",
    "summary": "",
    "translation": "基于解耦经典视觉描述符的图像表示探索",
    "relevance_score": 2,
    "reasoning": "该论文聚焦于纯粹的计算机视觉领域，研究传统视觉描述符的解耦表示方法，属于基础视觉特征提取技术。虽然视觉特征在多媒体搜索和推荐中有应用，但该研究缺乏与推荐系统、搜索或广告领域的直接关联，也未涉及LLM、Transformer架构或异质数据统一建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14532v1": {
    "title": "Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology",
    "url": "https://www.alphaxiv.org/abs/2510.14532v1",
    "arxiv_id": "2510.14532v1",
    "authors": "Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:24:23",
    "ori_summary": "Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.",
    "summary": "",
    "translation": "迈向牙科通用智能：面向口腔颌面放射学的视觉基础模型",
    "relevance_score": 1,
    "reasoning": "该论文聚焦于牙科医学领域的视觉基础模型应用，属于明确的医疗领域特定应用。虽然涉及基础模型技术，但其应用场景（口腔颌面放射学）与推荐系统、搜索或广告领域完全无关，违反了'医学、生物学、化学、物理学或其他领域特定应用'的排除规则。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14528v1": {
    "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
    "url": "https://www.alphaxiv.org/abs/2510.14528v1",
    "arxiv_id": "2510.14528v1",
    "authors": "Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:18:48",
    "ori_summary": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.",
    "summary": "",
    "translation": "PaddleOCR-VL：通过一个0.9B超紧凑视觉语言模型提升多语言文档解析能力",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视觉语言模型（VLM）技术，但其核心应用是文档解析，这属于计算机视觉领域的特定任务，与推荐系统、搜索或广告的排名和建模需求没有直接关联。超紧凑模型设计可能在效率方面有启发，但论文没有展示在异构数据处理或推荐/搜索场景中的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14526v1": {
    "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.14526v1",
    "arxiv_id": "2510.14526v1",
    "authors": "Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 10:14:34",
    "ori_summary": "In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model. While this pattern could output diverse images, some of them may fail to align well with the prompt. Existing methods alleviate this issue either by altering the denoising dynamics or by drawing multiple noises and conducting post-selection. In this paper, we attribute the misalignment to a training-inference mismatch: during training, prompt-conditioned noises lie in a prompt-specific subset of the latent space, whereas at inference the noise is drawn from a prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector that applies text-conditioned refinement to the initial noise before denoising. Conditioned on the prompt embedding, it maps the noise to a prompt-aware counterpart that better matches the distribution observed during SD training, without modifying the SD model. Our framework consists of these steps: we first sample some noises and obtain token-level feedback for their corresponding images from a vision-language model (VLM), then distill these signals into a reward model, and finally optimize the noise projector via a quasi-direct preference optimization. Our design has two benefits: (i) it requires no reference images or handcrafted priors, and (ii) it incurs small inference cost, replacing multi-sample selection with a single forward pass. Extensive experiments further show that our prompt-aware noise projection improves text-image alignment across diverse prompts.",
    "summary": "",
    "translation": "噪声投影：弥合扩散模型中文本到图像错位的提示无关差距",
    "relevance_score": 2,
    "reasoning": "该论文专注于扩散模型中文本到图像对齐的技术问题，这属于AIGC和内容生成领域，与我的核心关注点（推荐系统、搜索、广告）无关。虽然扩散模型是生成式AI的重要技术，但该研究主要解决图像生成中的对齐问题，没有明显的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14525v1": {
    "title": "Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing",
    "url": "https://www.alphaxiv.org/abs/2510.14525v1",
    "arxiv_id": "2510.14525v1",
    "authors": "Qurrat Ul Ain, Atif Aftab Ahmed Jilani, Zunaira Shafqat, Nigar Azhar Butt",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 10:14:32",
    "ori_summary": "Defective surgical instruments pose serious risks to sterility, mechanical integrity, and patient safety, increasing the likelihood of surgical complications. However, quality control in surgical instrument manufacturing often relies on manual inspection, which is prone to human error and inconsistency. This study introduces SurgScan, an AI-powered defect detection framework for surgical instruments. Using YOLOv8, SurgScan classifies defects in real-time, ensuring high accuracy and industrial scalability. The model is trained on a high-resolution dataset of 102,876 images, covering 11 instrument types and five major defect categories. Extensive evaluation against state-of-the-art CNN architectures confirms that SurgScan achieves the highest accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image, making it suitable for industrial deployment. Statistical analysis demonstrates that contrast-enhanced preprocessing significantly improves defect detection, addressing key limitations in visual inspection. SurgScan provides a scalable, cost-effective AI solution for automated quality control, reducing reliance on manual inspection while ensuring compliance with ISO 13485 and FDA standards, paving the way for enhanced defect detection in medical manufacturing.",
    "summary": "",
    "translation": "基于无损检测的实时手术器械缺陷检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医疗领域的手术器械检测，属于明确的医学应用范畴。论文内容涉及计算机视觉在医疗设备检测中的具体应用，与推荐系统、搜索、广告或相关使能技术没有任何关联，完全超出了当前关注范围。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14516v1": {
    "title": "Vision Mamba for Permeability Prediction of Porous Media",
    "url": "https://www.alphaxiv.org/abs/2510.14516v1",
    "arxiv_id": "2510.14516v1",
    "authors": "Ali Kashefi, Tapan Mukerji",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 10:02:33",
    "ori_summary": "Vision Mamba has recently received attention as an alternative to Vision Transformers (ViTs) for image classification. The network size of Vision Mamba scales linearly with input image resolution, whereas ViTs scale quadratically, a feature that improves computational and memory efficiency. Moreover, Vision Mamba requires a significantly smaller number of trainable parameters than traditional convolutional neural networks (CNNs), and thus, they can be more memory efficient. Because of these features, we introduce, for the first time, a neural network that uses Vision Mamba as its backbone for predicting the permeability of three-dimensional porous media. We compare the performance of Vision Mamba with ViT and CNN models across multiple aspects of permeability prediction and perform an ablation study to assess the effects of its components on accuracy. We demonstrate in practice the aforementioned advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous media. We make the source code publicly available to facilitate reproducibility and to enable other researchers to build on and extend this work. We believe the proposed framework has the potential to be integrated into large vision models in which Vision Mamba is used instead of ViTs.",
    "summary": "",
    "translation": "用于多孔介质渗透率预测的视觉Mamba模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于多孔介质渗透率预测，这是一个材料科学/地质工程领域的特定应用，与推荐系统、搜索或广告没有任何关联。虽然标题中提到了'Vision Mamba'（一种视觉架构），但该技术被应用于完全无关的物理科学领域，没有任何潜在的应用于RecSys/Search/Ads的可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14493v1": {
    "title": "Grazing Detection using Deep Learning and Sentinel-2 Time Series Data",
    "url": "https://www.alphaxiv.org/abs/2510.14493v1",
    "arxiv_id": "2510.14493v1",
    "authors": "Aleksis Pirinen, Delia Fano Yela, Smita Chakraborty, Erik Källman",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:37:43",
    "ori_summary": "Grazing shapes both agricultural production and biodiversity, yet scalable monitoring of where grazing occurs remains limited. We study seasonal grazing detection from Sentinel-2 L2A time series: for each polygon-defined field boundary, April-October imagery is used for binary prediction (grazed / not grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance features, and achieve an average F1 score of 77 percent across five validation splits, with 90 percent recall on grazed pastures. Operationally, if inspectors can visit at most 4 percent of sites annually, prioritising fields predicted by our model as non-grazed yields 17.2 times more confirmed non-grazing sites than random inspection. These results indicate that coarse-resolution, freely available satellite data can reliably steer inspection resources for conservation-aligned land-use compliance. Code and models have been made publicly available.",
    "summary": "",
    "translation": "基于深度学习和哨兵-2时间序列数据的放牧检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于遥感图像分析和农业应用中的放牧检测，使用卫星时间序列数据。这与推荐系统、搜索或广告领域完全无关，也不涉及任何LLM、Transformer技术或异构数据建模。该研究属于纯粹的遥感应用领域，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14463v1": {
    "title": "Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration",
    "url": "https://www.alphaxiv.org/abs/2510.14463v1",
    "arxiv_id": "2510.14463v1",
    "authors": "Thomas Katraouras, Dimitrios Rafailidis",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:04:05",
    "ori_summary": "Image quality is a critical factor in delivering visually appealing content on web platforms. However, images often suffer from degradation due to lossy operations applied by online social networks (OSNs), negatively affecting user experience. Image restoration is the process of recovering a clean high-quality image from a given degraded input. Recently, multi-task (all-in-one) image restoration models have gained significant attention, due to their ability to simultaneously handle different types of image degradations. However, these models often come with an excessively high number of trainable parameters, making them computationally inefficient. In this paper, we propose a strategy for compressing multi-task image restoration models. We aim to discover highly sparse subnetworks within overparameterized deep models that can match or even surpass the performance of their dense counterparts. The proposed model, namely MIR-L, utilizes an iterative pruning strategy that removes low-magnitude weights across multiple rounds, while resetting the remaining weights to their original initialization. This iterative process is important for the multi-task image restoration model's optimization, effectively uncovering \"winning tickets\" that maintain or exceed state-of-the-art performance at high sparsity levels. Experimental evaluation on benchmark datasets for the deraining, dehazing, and denoising tasks shows that MIR-L retains only 10% of the trainable parameters while maintaining high image restoration performance. Our code, datasets and pre-trained models are made publicly available at https://github.com/Thomkat/MIR-L.",
    "summary": "",
    "translation": "剪枝过参数化多任务网络用于退化网络图像恢复",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的图像恢复任务，属于图像处理范畴，与推荐系统、搜索或广告的核心技术关联性较弱。虽然网络剪枝技术可能对模型效率有普遍意义，但论文聚焦于web图像恢复这一特定应用场景，缺乏明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14462v1": {
    "title": "Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review",
    "url": "https://www.alphaxiv.org/abs/2510.14462v1",
    "arxiv_id": "2510.14462v1",
    "authors": "Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:02:52",
    "ori_summary": "Unsupervised deep generative models are emerging as a promising alternative to supervised methods for detecting and segmenting anomalies in brain imaging. Unlike fully supervised approaches, which require large voxel-level annotated datasets and are limited to well-characterised pathologies, these models can be trained exclusively on healthy data and identify anomalies as deviations from learned normative brain structures. This PRISMA-guided scoping review synthesises recent work on unsupervised deep generative models for anomaly detection in neuroimaging, including autoencoders, variational autoencoders, generative adversarial networks, and denoising diffusion models. A total of 49 studies published between 2018 - 2025 were identified, covering applications to brain MRI and, less frequently, CT across diverse pathologies such as tumours, stroke, multiple sclerosis, and small vessel disease. Reported performance metrics are compared alongside architectural design choices. Across the included studies, generative models achieved encouraging performance for large focal lesions and demonstrated progress in addressing more subtle abnormalities. A key strength of generative models is their ability to produce interpretable pseudo-healthy (also referred to as counterfactual) reconstructions, which is particularly valuable when annotated data are scarce, as in rare or heterogeneous diseases. Looking ahead, these models offer a compelling direction for anomaly detection, enabling semi-supervised learning, supporting the discovery of novel imaging biomarkers, and facilitating within- and cross-disease deviation mapping in unified end-to-end frameworks. To realise clinical impact, future work should prioritise anatomy-aware modelling, development of foundation models, task-appropriate evaluation metrics, and rigorous clinical validation.",
    "summary": "",
    "translation": "神经影像异常检测的无监督深度生成模型：系统性范围综述",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学神经影像领域的异常检测应用，属于明确的医疗领域特定应用。论文内容涉及深度生成模型在生物医学图像分析中的使用，与推荐系统、搜索或广告的核心技术领域完全无关，也不涉及任何可能应用于这些领域的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14460v1": {
    "title": "Structured Universal Adversarial Attacks on Object Detection for Video Sequences",
    "url": "https://www.alphaxiv.org/abs/2510.14460v1",
    "arxiv_id": "2510.14460v1",
    "authors": "Sven Jacob, Weijia Shao, Gjergji Kasneci",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 09:00:41",
    "ori_summary": "Video-based object detection plays a vital role in safety-critical applications. While deep learning-based object detectors have achieved impressive performance, they remain vulnerable to adversarial attacks, particularly those involving universal perturbations. In this work, we propose a minimally distorted universal adversarial attack tailored for video object detection, which leverages nuclear norm regularization to promote structured perturbations concentrated in the background. To optimize this formulation efficiently, we employ an adaptive, optimistic exponentiated gradient method that enhances both scalability and convergence. Our results demonstrate that the proposed attack outperforms both low-rank projected gradient descent and Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness. All code and data are publicly available at https://github.com/jsve96/AO-Exp-Attack.",
    "summary": "",
    "translation": "视频序列中针对目标检测的结构化通用对抗攻击",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的目标检测对抗攻击，属于纯粹的视觉安全研究。虽然目标检测在广告和推荐系统中可能有应用（如商品检测），但论文聚焦于对抗攻击这一安全相关主题，属于明确的无关主题范畴，与核心推荐系统、搜索广告技术进展或LLM/Transformer技术无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14431v1": {
    "title": "Real-Time Neural Video Compression with Unified Intra and Inter Coding",
    "url": "https://www.alphaxiv.org/abs/2510.14431v1",
    "arxiv_id": "2510.14431v1",
    "authors": "Hui Xiang, Yifan Bian, Li Li, Jingran Wu, Xianguo Zhang, Dong Liu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 08:31:44",
    "ori_summary": "Neural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 10.7\\% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.",
    "summary": "",
    "translation": "基于统一帧内与帧间编码的实时神经视频压缩",
    "relevance_score": 2,
    "reasoning": "该论文专注于神经视频压缩技术，属于计算机视觉领域的视频编码优化。虽然提到了实时性能和统一编码架构，但缺乏与推荐系统、搜索或广告的直接关联。视频压缩技术可能间接影响多媒体内容的存储和传输效率，但对核心的排序、匹配或用户建模任务没有明确的直接应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14427v1": {
    "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14427v1",
    "arxiv_id": "2510.14427v1",
    "authors": "Ho Yin Au, Jie Chen, Junkun Jiang, Jingyu Xiang",
    "categories": "cs.MM, cs.CV",
    "pub_date": "2025-10-16 08:28:46",
    "ori_summary": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.",
    "summary": "",
    "translation": "用于长运动序列生成的深度组合相位扩散方法",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于运动序列生成，属于计算机图形学或动画领域，与推荐系统、搜索或广告的核心技术无关。虽然扩散模型是LLM领域的一种技术，但该论文的应用场景（运动生成）与我的关注领域没有直接关联，也没有显示出在RecSys/Search/Ads中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14403v1": {
    "title": "DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis",
    "url": "https://www.alphaxiv.org/abs/2510.14403v1",
    "arxiv_id": "2510.14403v1",
    "authors": "Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 08:03:54",
    "ori_summary": "The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning model, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at https://github.com/tuuuc/DCMIL.",
    "summary": "",
    "translation": "DCMIL：一种用于癌症预后分析的全切片图像渐进式表征学习模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学领域的癌症预后分析，涉及全切片图像处理和医学影像分析。这属于明确的无关主题范畴（医学/生物学领域特定应用），与推荐系统、搜索、广告或相关使能技术没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14389v1": {
    "title": "BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble",
    "url": "https://www.alphaxiv.org/abs/2510.14389v1",
    "arxiv_id": "2510.14389v1",
    "authors": "Brandon Hill, Kma Solaiman",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2025-10-16 07:38:31",
    "ori_summary": "Motherboard defect detection is critical for ensuring reliability in high-volume electronics manufacturing. While prior research in PCB inspection has largely targeted bare-board or trace-level defects, assembly-level inspection of full motherboards inspection remains underexplored. In this work, we present BoardVision, a reproducible framework for detecting assembly-level defects such as missing screws, loose fan wiring, and surface scratches. We benchmark two representative detectors - YOLOv7 and Faster R-CNN, under controlled conditions on the MiracleFactory motherboard dataset, providing the first systematic comparison in this domain. To mitigate the limitations of single models, where YOLO excels in precision but underperforms in recall and Faster R-CNN shows the reverse, we propose a lightweight ensemble, Confidence-Temporal Voting (CTV Voter), that balances precision and recall through interpretable rules. We further evaluate robustness under realistic perturbations including sharpness, brightness, and orientation changes, highlighting stability challenges often overlooked in motherboard defect detection. Finally, we release a deployable GUI-driven inspection tool that bridges research evaluation with operator usability. Together, these contributions demonstrate how computer vision techniques can transition from benchmark results to practical quality assurance for assembly-level motherboard manufacturing.",
    "summary": "",
    "translation": "BoardVision：基于YOLO+Faster-RCNN集成方法的可部署且鲁棒的母板缺陷检测系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉在硬件缺陷检测领域的应用，属于纯粹的视觉检测任务。虽然采用了先进的检测架构（YOLO和Faster-RCNN），但其应用场景（主板缺陷检测）与推荐系统、搜索或广告领域没有任何关联，也不涉及任何可迁移到这些领域的技术方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14383v1": {
    "title": "DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights",
    "url": "https://www.alphaxiv.org/abs/2510.14383v1",
    "arxiv_id": "2510.14383v1",
    "authors": "Danish Ali, Ajmal Mian, Naveed Akhtar, Ghulam Mubashar Hassan",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:31:21",
    "ori_summary": "Accurate brain tumor segmentation is significant for clinical diagnosis and treatment. It is challenging due to the heterogeneity of tumor subregions. Mamba-based State Space Models have demonstrated promising performance. However, they incur significant computational overhead due to sequential feature computation across multiple spatial axes. Moreover, their robustness across diverse BraTS data partitions remains largely unexplored, leaving a critical gap in reliable evaluation. To address these limitations, we propose dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation model that captures multi-scale long-range dependencies with minimal computational overhead. We leverage a space-filling curve to preserve spatial locality during 3D-to-1D feature mapping, thereby reducing reliance on computationally expensive multi-axial feature scans. To enrich feature representation, we propose a gated fusion module that adaptively integrates forward and reverse contexts, along with a quantization block that discretizes features to improve robustness. In addition, we propose five systematic folds on BraTS2023 for rigorous evaluation of segmentation techniques under diverse conditions and present detailed analysis of common failure scenarios. On the 20\\% test set used by recent methods, our model achieves Dice improvements of 0.10\\% for whole tumor, 1.75\\% for tumor core, and 0.93\\% for enhancing tumor. Evaluations on the proposed systematic five folds demonstrate that our model maintains competitive whole tumor accuracy while achieving clear average Dice gains of 0.86\\% for tumor core and 1.45\\% for enhancing tumor over existing state-of-the-art. Furthermore, our model attains 15 times improvement in efficiency while maintaining high segmentation accuracy, highlighting its robustness and computational advantage over existing approaches.",
    "summary": "",
    "translation": "用于稳健高效脑肿瘤分割及分析洞察的DRBD-Mamba模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分割领域，特别是脑肿瘤分割，这属于明确的无关主题（医学/生物学应用）。虽然提到了Mamba架构（一种序列建模方法），但论文的应用场景和核心贡献都围绕医学图像分析，与推荐系统、搜索或广告领域没有任何直接或间接的关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14376v1": {
    "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
    "url": "https://www.alphaxiv.org/abs/2510.14376v1",
    "arxiv_id": "2510.14376v1",
    "authors": "Dongnam Byun, Jungwon Park, Jumgmin Ko, Changin Choi, Wonjong Rhee",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:17:23",
    "ori_summary": "Recent progress in text-to-image (T2I) generative models has led to significant improvements in generating high-quality images aligned with text prompts. However, these models still struggle with prompts involving multiple objects, often resulting in object neglect or object mixing. Through extensive studies, we identify four problematic scenarios, Similar Shapes, Similar Textures, Dissimilar Background Biases, and Many Objects, where inter-object relationships frequently lead to such failures. Motivated by two key observations about CLIP embeddings, we propose DOS (Directional Object Separation), a method that modifies three types of CLIP text embeddings before passing them into text-to-image models. Experimental results show that DOS consistently improves the success rate of multi-object image generation and reduces object mixing. In human evaluations, DOS significantly outperforms four competing methods, receiving 26.24%-43.04% more votes across four benchmarks. These results highlight DOS as a practical and effective solution for improving multi-object image generation.",
    "summary": "",
    "translation": "DOS：文本嵌入中的方向性对象分离用于多对象图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于多对象图像生成的计算机视觉任务，属于纯粹的视觉生成领域。虽然涉及文本嵌入技术，但其核心应用是图像生成而非推荐系统、搜索或广告中的排名或理解任务，与我的关注领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14374v1": {
    "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.14374v1",
    "arxiv_id": "2510.14374v1",
    "authors": "Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 07:16:18",
    "ori_summary": "Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR",
    "summary": "",
    "translation": "面向多模态大语言模型空间理解的空间偏好奖励",
    "relevance_score": 3,
    "reasoning": "该论文关注多模态大语言模型的空间理解能力，属于VLM（视觉语言模型）范畴。虽然VLM技术可能启发异构数据处理（如将用户序列和上下文特征视为不同模态），但论文标题未明确显示与推荐系统、搜索或广告的直接关联，且可能更偏向纯视觉理解而非实际应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14354v1": {
    "title": "Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration",
    "url": "https://www.alphaxiv.org/abs/2510.14354v1",
    "arxiv_id": "2510.14354v1",
    "authors": "Siddharth Tourani, Jayaram Reddy, Sarvesh Thakur, K Madhava Krishna, Muhammad Haris Khan, N Dinesh Reddy",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2025-10-16 06:47:10",
    "ori_summary": "With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has become available. This prompts the question of how to utilize this data for geometric reasoning of scenes. While many RGB-D registration meth- ods rely on geometric and feature-based similarity, we take a different approach. We use cycle-consistent keypoints as salient points to enforce spatial coherence constraints during matching, improving correspondence accuracy. Additionally, we introduce a novel pose block that combines a GRU recurrent unit with transformation synchronization, blending historical and multi-view data. Our approach surpasses previous self- supervised registration methods on ScanNet and 3DMatch, even outperforming some older supervised methods. We also integrate our components into existing methods, showing their effectiveness.",
    "summary": "",
    "translation": "利用循环一致性锚点进行自监督RGB-D配准",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的RGB-D配准技术，属于纯粹的视觉处理领域。虽然涉及自监督学习，但该方法在搜索、推荐或广告系统中没有明显的应用潜力，与异质数据统一建模或Transformer架构改进等关注点无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14349v1": {
    "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.14349v1",
    "arxiv_id": "2510.14349v1",
    "authors": "Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 06:38:39",
    "ori_summary": "Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM representations through Vision-Centric activation and Coordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted Token Gateway Mask (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks, showcasing its superior capabilities in visual comprehension.",
    "summary": "",
    "translation": "面向多模态大语言模型的视觉中心激活与协调机制",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态大语言模型的技术进展，属于'使能LLM技术'范畴。在推荐系统和搜索广告中，视觉中心激活与协调机制可应用于处理图像-文本多模态内容（如商品图片与描述），提升多模态特征融合和跨模态理解能力，从而改进商品推荐和搜索相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14340v1": {
    "title": "A Density-Informed Multimodal Artificial Intelligence Framework for Improving Breast Cancer Detection Across All Breast Densities",
    "url": "https://www.alphaxiv.org/abs/2510.14340v1",
    "arxiv_id": "2510.14340v1",
    "authors": "Siva Teja Kakileti, Bharath Govindaraju, Sudhakar Sampangi, Geetha Manjunath",
    "categories": "eess.IV, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-16 06:20:14",
    "ori_summary": "Mammography, the current standard for breast cancer screening, has reduced sensitivity in women with dense breast tissue, contributing to missed or delayed diagnoses. Thermalytix, an AI-based thermal imaging modality, captures functional vascular and metabolic cues that may complement mammographic structural data. This study investigates whether a breast density-informed multi-modal AI framework can improve cancer detection by dynamically selecting the appropriate imaging modality based on breast tissue composition. A total of 324 women underwent both mammography and thermal imaging. Mammography images were analyzed using a multi-view deep learning model, while Thermalytix assessed thermal images through vascular and thermal radiomics. The proposed framework utilized Mammography AI for fatty breasts and Thermalytix AI for dense breasts, optimizing predictions based on tissue type. This multi-modal AI framework achieved a sensitivity of 94.55% (95% CI: 88.54-100) and specificity of 79.93% (95% CI: 75.14-84.71), outperforming standalone mammography AI (sensitivity 81.82%, specificity 86.25%) and Thermalytix AI (sensitivity 92.73%, specificity 75.46%). Importantly, the sensitivity of Mammography dropped significantly in dense breasts (67.86%) versus fatty breasts (96.30%), whereas Thermalytix AI maintained high and consistent sensitivity in both (92.59% and 92.86%, respectively). This demonstrates that a density-informed multi-modal AI framework can overcome key limitations of unimodal screening and deliver high performance across diverse breast compositions. The proposed framework is interpretable, low-cost, and easily deployable, offering a practical path to improving breast cancer screening outcomes in both high-resource and resource-limited settings.",
    "summary": "",
    "translation": "一种基于密度信息的多模态人工智能框架，用于改善所有乳腺密度下的乳腺癌检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域的乳腺癌检测应用，属于医疗生物医学范畴。虽然提到了多模态AI框架，但其应用场景和问题领域完全在医疗诊断领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14314v1": {
    "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
    "url": "https://www.alphaxiv.org/abs/2510.14314v1",
    "arxiv_id": "2510.14314v1",
    "authors": "Shivangi Yadav, Arun Ross",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 05:21:30",
    "ori_summary": "An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method.",
    "summary": "",
    "translation": "用于虹膜呈现攻击检测的多域图像转换扩散风格GAN",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的虹膜识别和攻击检测，属于生物特征安全应用。虽然使用了扩散模型和GAN等生成技术，但内容与推荐系统、搜索或广告的核心技术无关，也不涉及Transformer架构改进或异构数据统一建模。该研究属于生物识别安全领域，属于明确的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14304v1": {
    "title": "Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding",
    "url": "https://www.alphaxiv.org/abs/2510.14304v1",
    "arxiv_id": "2510.14304v1",
    "authors": "Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2025-10-16 04:58:45",
    "ori_summary": "Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.",
    "summary": "",
    "translation": "面向事实性的水印技术：通过三层对比解码引导视觉语言模型趋向真实",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的事实性水印和真实性引导，属于幻觉缓解和模型安全范畴。虽然涉及视觉语言模型，但其核心焦点是事实性验证和水印技术，与推荐系统、搜索或广告中的排名、建模或效率改进没有直接关联，且水印技术被明确列为无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14293v1": {
    "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
    "url": "https://www.alphaxiv.org/abs/2510.14293v1",
    "arxiv_id": "2510.14293v1",
    "authors": "Yushi Du, Yixuan Li, Baoxiong Jia, Yutang Lin, Pei Zhou, Wei Liang, Yanchao Yang, Siyuan Huang",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2025-10-16 04:36:25",
    "ori_summary": "Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.",
    "summary": "",
    "translation": "学习人形机器人与人类的协作以实现协同物体搬运",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于机器人控制和人机协作领域，与推荐系统、搜索或广告的核心技术完全无关。论文涉及的是物理机器人协调和物体搬运任务，没有任何技术可以应用于RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14273v1": {
    "title": "CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts",
    "url": "https://www.alphaxiv.org/abs/2510.14273v1",
    "arxiv_id": "2510.14273v1",
    "authors": "Kieu-Anh Truong Thi, Huy-Hieu Pham, Duc-Trong Le",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:45:31",
    "ori_summary": "Domain shift in histopathology, often caused by differences in acquisition processes or data sources, poses a major challenge to the generalization ability of deep learning models. Existing methods primarily rely on modeling statistical correlations by aligning feature distributions or introducing statistical variation, yet they often overlook causal relationships. In this work, we propose a novel causal-inference-based framework that leverages semantic features while mitigating the impact of confounders. Our method implements the front-door principle by designing transformation strategies that explicitly incorporate mediators and observed tissue slides. We validate our method on the CAMELYON17 dataset and a private histopathology dataset, demonstrating consistent performance gains across unseen domains. As a result, our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and the private histopathology dataset, outperforming existing baselines. These results highlight the potential of causal inference as a powerful tool for addressing domain shift in histopathology image analysis.",
    "summary": "",
    "translation": "CLEAR：面向病理学肿瘤检测在分布外偏移下鲁棒性的因果学习框架",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学病理学领域的肿瘤检测，这是一个明确的医疗应用领域，属于明确的无关主题。论文标题中提到的分布外偏移和因果学习虽然是一般机器学习概念，但核心应用场景（病理学肿瘤检测）与推荐系统、搜索或广告领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14270v1": {
    "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
    "url": "https://www.alphaxiv.org/abs/2510.14270v1",
    "arxiv_id": "2510.14270v1",
    "authors": "Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2025-10-16 03:38:26",
    "ori_summary": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data. In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details. We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
    "summary": "",
    "translation": "GauSSmart：通过2D基础模型与几何滤波增强3D重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D重建的计算机视觉任务，属于纯粹的视觉技术范畴。虽然提到了基础模型，但其应用场景仅限于3D重建，与推荐系统、搜索或广告的核心技术领域没有直接关联，也不涉及Transformer架构改进或异构数据建模等相关技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14266v1": {
    "title": "Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment",
    "url": "https://www.alphaxiv.org/abs/2510.14266v1",
    "arxiv_id": "2510.14266v1",
    "authors": "Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:36:08",
    "ori_summary": "We propose a robust demodulation scheme for optical camera communication systems using an event-based vision sensor, combining OOK with toggle demodulation and a digital phase-locked loop. This is the first report to achieve a $\\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor experiments.",
    "summary": "",
    "translation": "长距离室外环境中基于事件的光学相机通信实验演示",
    "relevance_score": 1,
    "reasoning": "这篇论文关注光学相机通信技术，属于通信工程领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。该技术主要涉及物理层通信和信号处理，无法应用于用户建模、内容理解或排序算法等推荐/搜索/广告的核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14260v1": {
    "title": "MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching",
    "url": "https://www.alphaxiv.org/abs/2510.14260v1",
    "arxiv_id": "2510.14260v1",
    "authors": "Tingman Yan, Tao Liu, Xilian Yang, Qunfei Zhao, Zeyang Xia",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:21:28",
    "ori_summary": "Cross-view matching is fundamentally achieved through cross-attention mechanisms. However, matching of high-resolution images remains challenging due to the quadratic complexity and lack of explicit matching constraints in the existing cross-attention. This paper proposes an attention mechanism, MatchAttention, that dynamically matches relative positions. The relative position determines the attention sampling center of the key-value pairs given a query. Continuous and differentiable sliding-window attention sampling is achieved by the proposed BilinearSoftmax. The relative positions are iteratively updated through residual connections across layers by embedding them into the feature channels. Since the relative position is exactly the learning target for cross-view matching, an efficient hierarchical cross-view decoder, MatchDecoder, is designed with MatchAttention as its core component. To handle cross-view occlusions, gated cross-MatchAttention and a consistency-constrained loss are proposed. These two components collectively mitigate the impact of occlusions in both forward and backward passes, allowing the model to focus more on learning matching relationships. When applied to stereo matching, MatchStereo-B ranked 1st in average error on the public Middlebury benchmark and requires only 29ms for KITTI-resolution inference. MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU memory. The proposed models also achieve state-of-the-art performance on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high accuracy and low computational complexity makes real-time, high-resolution, and high-accuracy cross-view matching possible. Code is available at https://github.com/TingmanYan/MatchAttention.",
    "summary": "",
    "translation": "MatchAttention：通过匹配相对位置实现高分辨率跨视图匹配",
    "relevance_score": 6,
    "reasoning": "该论文提出了一种新的注意力机制用于跨视图匹配，这属于Transformer架构的进步（Enabling Transformer Tech）。在推荐系统或搜索中，这种技术可以应用于多模态内容匹配、跨域推荐或异构数据对齐，通过改进位置感知的跨视图匹配来提升模型性能。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2510.14256v1": {
    "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.14256v1",
    "arxiv_id": "2510.14256v1",
    "authors": "Xiangyu Meng, Zixian Zhang, Zhenghao Zhang, Junchao Liao, Long Qin, Weizhi Wang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:13:56",
    "ori_summary": "While advanced methods like VACE and Phantom have advanced video generation for specific subjects in diverse scenarios, they struggle with multi-human identity preservation in dynamic interactions, where consistent identities across multiple characters are critical. To address this, we propose Identity-GRPO, a human feedback-driven optimization pipeline for refining multi-human identity-preserving video generation. First, we construct a video reward model trained on a large-scale preference dataset containing human-annotated and synthetic distortion data, with pairwise annotations focused on maintaining human consistency throughout the video. We then employ a GRPO variant tailored for multi-human consistency, which greatly enhances both VACE and Phantom. Through extensive ablation studies, we evaluate the impact of annotation quality and design choices on policy optimization. Experiments show that Identity-GRPO achieves up to 18.9% improvement in human consistency metrics over baseline methods, offering actionable insights for aligning reinforcement learning with personalized video generation.",
    "summary": "",
    "translation": "Identity-GRPO：通过强化学习优化多人物身份保持的视频生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频生成中的身份保持问题，这属于计算机视觉和生成式AI领域，与推荐系统、搜索或广告的核心技术关联性较弱。虽然使用了强化学习，但应用场景是视频生成而非推荐/搜索/广告中的排序或建模任务，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14255v1": {
    "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
    "url": "https://www.alphaxiv.org/abs/2510.14255v1",
    "arxiv_id": "2510.14255v1",
    "authors": "Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, Bo Zheng",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:13:47",
    "ori_summary": "Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
    "summary": "",
    "translation": "通过奖励引导优化实现身份保持的图像到视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像到视频生成技术，属于计算机视觉领域的AIGC内容生成范畴，与推荐系统、搜索或广告的核心技术无关。虽然提到了奖励引导优化，但这主要用于视频生成质量提升，没有明确的推荐、搜索或广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14251v1": {
    "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering",
    "url": "https://www.alphaxiv.org/abs/2510.14251v1",
    "arxiv_id": "2510.14251v1",
    "authors": "Mingkai Liu, Dikai Fan, Haohua Que, Haojia Gao, Xiao Liu, Shuxue Peng, Meixia Lin, Shengyu Gu, Ruicong Ye, Wanli Qiu, Handong Yao, Ruopeng Zhang, Xianliang Huang",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 03:08:19",
    "ori_summary": "Efficient localization and high-quality rendering in large-scale scenes remain a significant challenge due to the computational cost involved. While Scene Coordinate Regression (SCR) methods perform well in small-scale localization, they are limited by the capacity of a single network when extended to large-scale scenes. To address these challenges, we propose the Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables efficient localization and high-quality rendering in large-scale scenes. Inspired by the remarkable capabilities of MOE in large model domains, we introduce a gating network to implicitly classify and select sub-networks, ensuring that only a single sub-network is activated during each inference. Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to enhance the localization accuracy on large-scale scene. Our framework provides a significant reduction in costs while maintaining higher precision, offering an efficient solution for large-scale scene applications. Additional experiments on the Cambridge test set demonstrate that our method achieves high-quality rendering results with merely 10 minutes of training.",
    "summary": "",
    "translation": "MACE：基于专家混合加速坐标编码的大规模场景定位与渲染",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的场景定位和渲染技术，虽然提到了专家混合（MoE）架构，但其应用场景是视觉定位和渲染，而非推荐系统、搜索或广告领域。MoE技术本身具有效率优势，但该论文没有展示其在推荐/搜索/广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14245v1": {
    "title": "Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication",
    "url": "https://www.alphaxiv.org/abs/2510.14245v1",
    "arxiv_id": "2510.14245v1",
    "authors": "Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama",
    "categories": "cs.CV",
    "pub_date": "2025-10-16 02:56:29",
    "ori_summary": "Optical camera communication (OCC) represents a promising visible light communication technology. Nonetheless, typical OCC systems utilizing frame-based cameras are encumbered by limitations, including low bit rate and high processing load. To address these issues, OCC system utilizing an event-based vision sensor (EVS) as receivers have been proposed. The EVS enables high-speed, low-latency, and robust communication due to its asynchronous operation and high dynamic range. In existing event-based OCC systems, conventional modulation schemes such as on-off keying (OOK) and pulse position modulation have been applied, however, to the best of our knowledge, no modulation method has been proposed that fully exploits the unique characteristics of the EVS. This paper proposes a novel modulation scheme, called the event interval modulation (EIM) scheme, specifically designed for event-based OCC. EIM enables improvement in transmission speed by modulating information using the intervals between events. This paper proposes a theoretical model of EIM and conducts a proof-of-concept experiment. First, the parameters of the EVS are tuned and customized to optimize the frequency response specifically for EIM. Then, the maximum modulation order usable in EIM is determined experimentally. We conduct transmission experiments based on the obtained parameters. Finally, we report successful transmission at 28 kbps over 10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new benchmark for bit rate in event-based OCC systems.",
    "summary": "",
    "translation": "事件间隔调制：一种基于事件的光学相机通信新方案",
    "relevance_score": 1,
    "reasoning": "该论文涉及事件相机和光学通信技术，属于计算机视觉和通信工程领域。与推荐系统、搜索或广告的核心技术焦点完全无关，也没有任何明显的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.14244v1": {
    "title": "Reinforcement Learning for Unsupervised Domain Adaptation in Spatio-Temporal Echocardiography Segmentation",
    "url": "https://www.alphaxiv.org/abs/2510.14244v1",
    "arxiv_id": "2510.14244v1",
    "authors": "Arnaud Judge, Nicolas Duchateau, Thierry Judge, Roman A. Sandler, Joseph Z. Sokol, Christian Desrosiers, Olivier Bernard, Pierre-Marc Jodoin",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2025-10-16 02:55:04",
    "ori_summary": "Domain adaptation methods aim to bridge the gap between datasets by enabling knowledge transfer across domains, reducing the need for additional expert annotations. However, many approaches struggle with reliability in the target domain, an issue particularly critical in medical image segmentation, where accuracy and anatomical validity are essential. This challenge is further exacerbated in spatio-temporal data, where the lack of temporal consistency can significantly degrade segmentation quality, and particularly in echocardiography, where the presence of artifacts and noise can further hinder segmentation performance. To address these issues, we present RL4Seg3D, an unsupervised domain adaptation framework for 2D + time echocardiography segmentation. RL4Seg3D integrates novel reward functions and a fusion scheme to enhance key landmark precision in its segmentations while processing full-sized input videos. By leveraging reinforcement learning for image segmentation, our approach improves accuracy, anatomical validity, and temporal consistency while also providing, as a beneficial side effect, a robust uncertainty estimator, which can be used at test time to further enhance segmentation performance. We demonstrate the effectiveness of our framework on over 30,000 echocardiographic videos, showing that it outperforms standard domain adaptation techniques without the need for any labels on the target domain. Code is available at https://github.com/arnaudjudge/RL4Seg3D.",
    "summary": "",
    "translation": "强化学习用于时空超声心动图分割的无监督域自适应",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像分割领域的强化学习应用，属于明确的医学领域特定应用。虽然涉及强化学习技术，但应用于超声心动图分割这一医疗场景，与推荐系统、搜索或广告领域没有任何关联，完全属于被排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}